{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de78e7d",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "Take all the raw data and upload it to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "81883de1-5183-4d3f-be1a-2c9361324b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "from myfuncs import explore_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07525fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4731fd0",
   "metadata": {},
   "source": [
    "## Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0163dade",
   "metadata": {},
   "source": [
    "- A table for all unique companies\n",
    "    - cik, symbol, name, sector, subsector, founded, etc\n",
    "- A table for all the periods a company has been on the index.\n",
    "    - cik, symbol, start date, end date, flag_current"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f4bc4",
   "metadata": {},
   "source": [
    "#### Table for unique companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b89e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import companies csv\n",
    "current_companies = pd.read_csv('../data/raw/companies_wiki.csv').drop(columns='SEC filings')\n",
    "current_companies.columns = ['symbol', 'name', 'sector', 'subSector', 'hQ', 'dateFirstAdded', 'cik', 'founded']\n",
    "# import historical companies csv\n",
    "historical_v1 = pd.read_csv('../data/raw/historical_companies_wiki.csv')\n",
    "# import wikipedia historical companies csv\n",
    "spts = pd.read_csv('../data/raw/historical_companies_TradingEvolved.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801fe6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tickers</th>\n",
       "      <th>tickers_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...</td>\n",
       "      <td>[AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-01-03</td>\n",
       "      <td>AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...</td>\n",
       "      <td>[AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-01-04</td>\n",
       "      <td>AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...</td>\n",
       "      <td>[AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-01-10</td>\n",
       "      <td>AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...</td>\n",
       "      <td>[AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1996-01-11</td>\n",
       "      <td>AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...</td>\n",
       "      <td>[AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                            tickers  \\\n",
       "0  1996-01-02  AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...   \n",
       "1  1996-01-03  AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...   \n",
       "2  1996-01-04  AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...   \n",
       "3  1996-01-10  AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...   \n",
       "4  1996-01-11  AAL,AAMRQ,AAPL,ABI,ABS,ABT,ABX,ACKH,ACV,ADM,AD...   \n",
       "\n",
       "                                    tickers_filtered  \n",
       "0  [AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...  \n",
       "1  [AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...  \n",
       "2  [AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...  \n",
       "3  [AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...  \n",
       "4  [AAL, AAMRQ, AAPL, ABI, ABS, ABT, ABX, ACKH, A...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique occurences of anything that is in tickers\n",
    "# get the items on the list if there is no '-', if there is, get the first item (the ticker)\n",
    "spts['tickers_filtered'] = spts.tickers.str.split(',')\n",
    "spts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb622b",
   "metadata": {},
   "source": [
    "Get all unique constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa69fc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = set()\n",
    "spts.tickers_filtered.apply(results.update)\n",
    "companies = pd.DataFrame(data = results, columns=['symbol'])\n",
    "companies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3dff01",
   "metadata": {},
   "source": [
    "Add values from current companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af828914",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_companies['currentConstituent'] = True\n",
    "companies = companies.merge(current_companies, how='left')\n",
    "companies = companies.drop(columns=['dateFirstAdded'])\n",
    "companies.currentConstituent = companies.currentConstituent.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5d68712",
   "metadata": {},
   "outputs": [],
   "source": [
    "ciks = pd.read_csv('../data/raw/CIK.csv', index_col = 0)\n",
    "ciks.columns = ['cik_sec_list', 'symbol', 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "739e101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = companies.merge(ciks, how='left')\n",
    "companies.name = companies.name.fillna(companies.title)\n",
    "companies.cik = companies.cik.fillna(companies.cik_sec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32176f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.to_csv('../data/preSQL/companies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ef7c7",
   "metadata": {},
   "source": [
    "#### Table for historical constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a19ee1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_stays = pd.DataFrame(columns=['symbol', 'added', 'removed'])\n",
    "# iterate over the list of tickers for every day and\n",
    "previous_set=set()\n",
    "for date, list_tickers in spts.set_index('date').tickers_filtered.items():\n",
    "    # for the first iteration there is no previous set\n",
    "    new_set = set(list_tickers)\n",
    "    # check which values from the list of values was not in the previous date\n",
    "    diff_new = new_set-previous_set\n",
    "    # for each value in diff_new\n",
    "    for diff_ticker in diff_new:\n",
    "        if diff_ticker in new_set:\n",
    "            # the ticker has been added\n",
    "            new_row_index = 0 if len(sp500_stays) == 0 else sp500_stays.index.max()+1\n",
    "            new_row_data={\n",
    "                'symbol': diff_ticker,\n",
    "                'added': date,\n",
    "                'removed': 'not_yet_removed',\n",
    "                }\n",
    "            new_row = pd.DataFrame(data = new_row_data, index=[new_row_index])\n",
    "            sp500_stays = pd.concat([sp500_stays, new_row], axis=0)\n",
    "    diff_old = previous_set-new_set\n",
    "    for diff_ticker in diff_old:\n",
    "        if diff_ticker in previous_set:\n",
    "            # the ticker has been removed\n",
    "            # get the index of the last occurence of the ticker in the dataframe\n",
    "            mask = sp500_stays.symbol == diff_ticker\n",
    "            idx = sp500_stays[mask].index.max()\n",
    "            # update that index with the date removed\n",
    "            sp500_stays.loc[idx, 'removed'] = date\n",
    "    # this iteration ends, the new set becomes obsolete\n",
    "    previous_set = new_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d40f2f",
   "metadata": {},
   "source": [
    "to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e872e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_stays.to_csv('../data/preSQL/sp500_movements.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27657b",
   "metadata": {},
   "source": [
    "## SEC submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb6d6f",
   "metadata": {},
   "source": [
    "Submissions from SEC\n",
    "- 10-KA/405A and 10QA are text amendments which contain no financial information. <a href=\"https://www.sec.gov/Archives/edgar/data/320193/0001047469-98-001822.txt\">example</a>\n",
    "- NT 10-Q and NT 10-K are notifications about delay in statements\n",
    "- 10KT and 10QT dennote transition in companies which alter fiscal years. Usually after merger of acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions=pd.read_csv('.././data/raw/submissions.csv')\n",
    "sub_cols = [\n",
    "    'filingDate',\n",
    "    'reportDate',\n",
    "    'symbol',\n",
    "    'cik',\n",
    "    'form',\n",
    "    ]\n",
    "sec = submissions.loc[:,sub_cols]\n",
    "mask = sec.form.isin(['10-K', '10-Q', '10-K405', '10-KT', '10-QT'])\n",
    "sec = sec[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb73497",
   "metadata": {},
   "source": [
    "## 10-K Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107806a",
   "metadata": {},
   "source": [
    "#### Walk the path in a directory and generate the dataframe.\n",
    "- We're only interested in dates when the symbol belonged to the sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cdae054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(path):\n",
    "    df_list = []\n",
    "    path_dir = Path(path)\n",
    "    sp500_dates = pd.read_csv('../data/raw/sp500_movements.csv')\n",
    "    sp500_dates.removed = sp500_dates.removed.replace('not_yet_removed', '2022-12-31')\n",
    "    sp500_dates.added = pd.to_datetime(sp500_dates.added)\n",
    "    sp500_dates.removed = pd.to_datetime(sp500_dates.removed)\n",
    "    for file in path_dir.glob('*.csv'):\n",
    "        csv_path = os.path.join(file.parent, file.name)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # convert fillingDates to datetime\n",
    "        df.fillingDate = pd.to_datetime(df.fillingDate)\n",
    "        # keep only the the combinations of symbol and date which belonged to the sp500\n",
    "        mask = sp500_dates.symbol == file.name.split('.')[0]\n",
    "        added = sp500_dates[mask].added\n",
    "        removed = sp500_dates[mask].removed\n",
    "        for stay in tuple(zip(added, removed)):\n",
    "            df_list.append(df[df.fillingDate.between(stay[0], stay[1])])\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bf4a29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_merge():\n",
    "    balance = merge_csv('../data/raw/balance')\n",
    "    balance.to_csv('../data/raw/balance.csv', index=False)\n",
    "    print(balance.shape)\n",
    "\n",
    "    cash_flow = merge_csv('../data/raw/cash_flow')\n",
    "    cash_flow.to_csv('../data/raw/cash_flow.csv', index=False)\n",
    "    print(cash_flow.shape)\n",
    "\n",
    "    income = merge_csv('../data/raw/income')\n",
    "    income.to_csv('../data/raw/income.csv', index=False)\n",
    "    print(income.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c809ba09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10179, 54)\n",
      "(10309, 40)\n",
      "(10328, 38)\n"
     ]
    }
   ],
   "source": [
    "do_merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003cc8a",
   "metadata": {},
   "source": [
    "#### Table for the join of all the historical financial statements in the SP500\n",
    "- Clean the primary keys: symbol + year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39593cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6724901",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = pd.read_csv('../data/raw/balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a4652fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance.fillingDate.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d0e2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65b32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bafa69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9e33f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flow = pd.read_csv('../data/raw/cash_flow.csv', index_col = 0)\n",
    "income = pd.read_csv('../data/raw/income.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48212c7",
   "metadata": {},
   "source": [
    "Join cash flow and income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joincols = ['symbol', 'calendarYear']\n",
    "left_drop = ['netIncome', 'depreciationAndAmortization']\n",
    "right_drop = [\n",
    "    'date', \n",
    "    'reportedCurrency',\n",
    "    'cik',\n",
    "    'fillingDate',\n",
    "    'acceptedDate',\n",
    "    'period',\n",
    "    'link',\n",
    "    'finalLink']\n",
    "statements = (\n",
    "    cflow.drop(columns=left_drop,)\n",
    "    .merge(income.drop(columns=right_drop),on=joincols,how='inner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250e08c",
   "metadata": {},
   "source": [
    "Join cflow+income and balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joincols = ['symbol', 'calendarYear']\n",
    "right_drop = [\n",
    "    'date', \n",
    "    'reportedCurrency',\n",
    "    'cik',\n",
    "    'fillingDate',\n",
    "    'acceptedDate',\n",
    "    'period',\n",
    "    'inventory',\n",
    "    'link',\n",
    "    'finalLink']\n",
    "statements = statements.merge(balance.drop(columns=right_drop),on=joincols,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements.to_csv('.././data/feng/statements_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc4d94",
   "metadata": {},
   "source": [
    "## market cap\n",
    "- Read raw data from API\n",
    "- Convert to billions\n",
    "- Fix market cap severe mistakes (order of magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997752f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcaps = pd.read_csv('.././data/raw/mcaps.csv').convert_dtypes()\n",
    "mcaps['date'] = pd.to_datetime(mcaps['date'])\n",
    "# convert to billions\n",
    "mcaps['marketCap'] = mcaps['marketCap']/1e9\n",
    "# get the absolute value of negatives\n",
    "mcaps['marketCap'] = abs(mcaps['marketCap'])\n",
    "# get the companies valued at more than 1 Trillion which should be\n",
    "giants = [ 'GOOGL', 'GOOG', 'AMZN', 'FB', 'MSFT', 'AAPL', 'TSLA']\n",
    "cond1 = mcaps.marketCap > 1e3\n",
    "cond2 = ~mcaps.symbol.isin(giants)\n",
    "aux = mcaps[cond1&cond2].sort_values(by=['marketCap', 'symbol', 'date'])\n",
    "# SHW: /1e9\n",
    "idx = aux[aux.symbol=='SHW'].index\n",
    "mcaps.loc[idx, 'marketCap'] = mcaps.loc[idx, 'marketCap']/ 1e6\n",
    "# REST: / 1e6\n",
    "idx = aux[~(aux.symbol=='SHW')].index\n",
    "mcaps.loc[idx, 'marketCap'] = mcaps.loc[idx, 'marketCap']/ 1e3\n",
    "# GOOG wrong values\n",
    "mcaps.loc[161468:161502, 'marketCap'] = mcaps.loc[161468:161502, 'marketCap'] + 1000\n",
    "mcaps.loc[161656:161659, 'marketCap'] = mcaps.loc[161656:161659, 'marketCap'] * 2\n",
    "mcaps.loc[161621:161649, 'marketCap'] = mcaps.loc[161621:161649, 'marketCap'] * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f94453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to feature engineering\n",
    "mcaps.to_csv('.././data/feng/mcaps.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8ec8f4bfe78eccfa1d1de3a91d61a30d0bd8efad5c2f5d61c7b5760b6b34097"
  },
  "kernelspec": {
   "display_name": "sp500",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
