{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer, calinski_harabasz_score, silhouette_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.cluster import KMeans\n",
    "# bayestian hyperparameter tunning\n",
    "import optuna\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "# models to try\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from myvars import input_features\n",
    "from myfuncs import num_describe, generate_features\n",
    "from myclasses import Windsorizer\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 200\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data():\n",
    "    data = pd.read_csv('../data/processed/data.csv')\n",
    "    data = data.dropna(subset=input_features)\n",
    "    data = generate_features(data)\n",
    "    data.query(\"calendarYear > 2018\").to_csv('../data/processed/future.csv', index=False)\n",
    "    data.query(\"calendarYear <= 2018\").to_csv('../data/processed/present.csv', index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[feature_name] = data[col]/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features['netSharesRepurchasedToRevenue'] = net_shares/data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[yoy_name] = features.groupby('symbol')[colname].pct_change(1)\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[trend_name] = features.groupby('symbol', as_index=False)[colname].rolling(window=3, min_periods=1).mean()[colname]\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features['freeCashFlowGivenToShareholders'] = (net_shares + abs(data.dividendsPaid)) / data.freeCashFlow # careful negative cash flow issuing stock\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features['PPEtoSales'] = data.propertyPlantEquipmentNet / data.revenue\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features['netDebtToEBITDA'] = (data.totalDebt - true_cash) / data.ebitda\n",
      "c:\\Users\\fpala\\2022\\sp500\\src\\notebooks\\myfuncs.py:183: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features['roe'] = data.netIncome / data.totalStockholdersEquity\n"
     ]
    }
   ],
   "source": [
    "separate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed/present.csv')\n",
    "features = data.drop(columns=['target', 'symbol', 'calendarYear', 'fillingDate'])\n",
    "target = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5925, 76), (1975, 76), (5925,), (1975,)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    np.log(target), \n",
    "    test_size=0.25, \n",
    "    random_state = 46)\n",
    "[x.shape for x in [X_train, X_test, y_train, y_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train,y_train], axis=1).to_csv('../data/processed/train.csv', index=False)\n",
    "pd.concat([X_test,y_test], axis=1).to_csv('../data/processed/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4624.240947756172"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(y_test)\n",
    "mean = y_train.mean()\n",
    "worst_preds = np.tile(mean, n)\n",
    "mean_squared_error(y_true=np.exp(y_test), y_pred = np.exp(worst_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_linear_regression(pipe, X_train, X_test, y_train, y_test=None):\n",
    "    pipe.steps.append(('linear_regression', LinearRegression()))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds_test = pipe.predict(X_test)\n",
    "    preds_train = pipe.predict(X_train)\n",
    "    mse_test = mean_squared_error(y_true=np.exp(y_test), y_pred=np.exp(preds_test))\n",
    "    mse_train = mean_squared_error(y_true=np.exp(y_train), y_pred=np.exp(preds_train))\n",
    "    print('mse train:', mse_train)\n",
    "    print('mse test: ', mse_test)\n",
    "    print('rmse test: ', np.sqrt(mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 725.4274788642791\n",
      "mse test:  560.2357481576806\n",
      "rmse test:  23.669299697238205\n"
     ]
    }
   ],
   "source": [
    "mypipe = Pipeline(steps=[\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ])\n",
    "do_linear_regression(mypipe, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 290.9250096950543\n",
      "mse test:  3.965944527788969e+120\n",
      "rmse test:  1.991467932905014e+60\n"
     ]
    }
   ],
   "source": [
    "def do_poly_regression(pipe, X_train, X_test, y_train, y_test=None, degree = 2):\n",
    "    pipe.steps.append(('poly_transform', PolynomialFeatures(degree=degree)))\n",
    "    pipe.steps.append(('regression', LinearRegression()))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds_test = pipe.predict(X_test)\n",
    "    preds_train = pipe.predict(X_train)\n",
    "    mse_test = mean_squared_error(y_true=np.exp(y_test), y_pred=np.exp(preds_test))\n",
    "    mse_train = mean_squared_error(y_true=np.exp(y_train), y_pred=np.exp(preds_train))\n",
    "    print('mse train:', mse_train)\n",
    "    print('mse test: ', mse_test)\n",
    "    print('rmse test: ', np.sqrt(mse_test))\n",
    "\n",
    "mypipe = Pipeline(steps=[\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ])\n",
    "do_poly_regression(mypipe, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Insane Overfit**. 1e113 order of magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 692.0856264052034\n",
      "mse test:  566.0197971690108\n",
      "rmse test:  23.791170571642976\n"
     ]
    }
   ],
   "source": [
    "def do_linear_svm_regression(pipe, X_train, X_test, y_train, y_test=None):\n",
    "    pipe.steps.append(('linear_svm', LinearSVR(C=0.02, max_iter=1000)))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds_test = pipe.predict(X_test)\n",
    "    preds_train = pipe.predict(X_train)\n",
    "    mse_test = mean_squared_error(y_true=np.exp(y_test), y_pred=np.exp(preds_test))\n",
    "    mse_train = mean_squared_error(y_true=np.exp(y_train), y_pred=np.exp(preds_train))\n",
    "    print('mse train:', mse_train)\n",
    "    print('mse test: ', mse_test)\n",
    "    print('rmse test: ', np.sqrt(mse_test))\n",
    "\n",
    "mypipe = Pipeline(steps=[\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ])\n",
    "do_linear_svm_regression(mypipe, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check ill-conditioning. For C's larger than 0.02 it stops converging despite max_iter being 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with radial basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 417.7711114964257\n",
      "mse test:  567.7562519327167\n",
      "rmse test:  23.827636306035828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', PowerTransformer()), ('rbf_svm', SVR(C=1.5))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_svm_regression(pipe, X_train, X_test, y_train, y_test=None):\n",
    "    pipe.steps.append(('rbf_svm', SVR(kernel = 'rbf', C=1.5, epsilon=0.1)))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds_test = pipe.predict(X_test)\n",
    "    preds_train = pipe.predict(X_train)\n",
    "    mse_test = mean_squared_error(y_true=np.exp(y_test), y_pred=np.exp(preds_test))\n",
    "    mse_train = mean_squared_error(y_true=np.exp(y_train), y_pred=np.exp(preds_train))\n",
    "    print('mse train:', mse_train)\n",
    "    print('mse test: ', mse_test)\n",
    "    print('rmse test: ', np.sqrt(mse_test))\n",
    "    return pipe\n",
    "\n",
    "mypipe = Pipeline(steps=[\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ])\n",
    "do_svm_regression(mypipe, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 602.5579938237018\n",
      "mse test:  636.0603936293599\n",
      "rmse test:  25.22023777900121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', PowerTransformer()),\n",
       "                ('rfe',\n",
       "                 RFE(estimator=LinearRegression(), n_features_to_select=5)),\n",
       "                ('knn', KNeighborsRegressor())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_neighbors(pipe, X_train, X_test, y_train, y_test=None):\n",
    "    pipe.steps.append(('rfe', RFE(estimator = LinearRegression(), n_features_to_select = 5)))\n",
    "    pipe.steps.append(('knn', KNeighborsRegressor(n_neighbors=5)))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds_test = pipe.predict(X_test)\n",
    "    preds_train = pipe.predict(X_train)\n",
    "    mse_test = mean_squared_error(y_true=np.exp(y_test), y_pred=np.exp(preds_test))\n",
    "    mse_train = mean_squared_error(y_true=np.exp(y_train), y_pred=np.exp(preds_train))\n",
    "    print('mse train:', mse_train)\n",
    "    print('mse test: ', mse_test)\n",
    "    print('rmse test: ', np.sqrt(mse_test))\n",
    "    return pipe\n",
    "\n",
    "mypipe = Pipeline(steps=[\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ])\n",
    "do_neighbors(mypipe, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "- Including some basic hyperparameter tunning trying skopt interface for BayesSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "mse train: 292.47084581120157\n",
      "mse test:  428.3632211142772\n",
      "rmse test:  20.696937481527968\n"
     ]
    }
   ],
   "source": [
    "def search_forest(pipe, X_train, X_test, y_train, y_test=None):\n",
    "    model = RandomForestRegressor()\n",
    "    param_grid={\n",
    "        'max_depth':(3,10),\n",
    "        'min_samples_leaf':(1e-3, 0.1, 'log-uniform'),\n",
    "        'min_samples_split':(1e-3, 0.1, 'log-uniform'),\n",
    "    }\n",
    "    pipe.steps.append(('forest_optimizer', BayesSearchCV(model, search_spaces=param_grid, n_iter = 5, n_jobs=5,n_points = 5, verbose = 10, )))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds_test = pipe.predict(X_test)\n",
    "    preds_train = pipe.predict(X_train)\n",
    "    mse_test = mean_squared_error(y_true=np.exp(y_test), y_pred=np.exp(preds_test))\n",
    "    mse_train = mean_squared_error(y_true=np.exp(y_train), y_pred=np.exp(preds_train))\n",
    "    print('mse train:', mse_train)\n",
    "    print('mse test: ', mse_test)\n",
    "    print('rmse test: ', np.sqrt(mse_test))\n",
    "    return pipe\n",
    "\n",
    "mypipe = Pipeline(steps=[\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ])\n",
    "mypipe = search_forest(mypipe, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('max_depth', 9),\n",
       "              ('min_samples_leaf', 0.0010256600861588028),\n",
       "              ('min_samples_split', 0.0023241843515524204)]),\n",
       " 0.912892830173391)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = mypipe['forest_optimizer']\n",
    "a.best_params_, a.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High overfitting because RF has no regularization process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "- Change scorer to a better suited one. Abandoned, takes ages, skopt wasn't working out, back to optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score(y, y_pred, **kwargs):\n",
    "    return mean_squared_error(y_true=np.exp(y), y_pred=np.exp(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_boosting(pipe, X_train, X_test, y_train, y_test=None):\n",
    "    def objective(trial):\n",
    "        params={\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"booster\": \"gbtree\",\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 20, 500),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-6, 1.0, log=True),\n",
    "            \"max_depth\" : trial.suggest_int(\"max_depth\", 5, 10),\n",
    "            \"colsample_bytree\" : trial.suggest_float(\"colsample_bytree\", 0.4, 1),\n",
    "            \"subsample\" : trial.suggest_float(\"subsample\", 0.5, 1),\n",
    "            \"eta\" : trial.suggest_float(\"eta\", 1e-2, 0.2, log=True)\n",
    "            }\n",
    "        pipe = Pipeline(steps=[\n",
    "            ('scaler', PowerTransformer()),\n",
    "            ('xgb', XGBRegressor(**params)),\n",
    "            ])\n",
    "        pipe.fit(X_train,y_train)\n",
    "        preds = pipe.predict(X_test)\n",
    "        mse = mean_squared_error(y_true=np.exp(y_test), y_pred=np.exp(preds))\n",
    "        return mse\n",
    "\n",
    "    minutes = 20\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, timeout=(60*minutes))\n",
    "    results = study.trials_dataframe()\n",
    "    return results, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286.40738209295756,\n",
       " {'min_child_weight': 58,\n",
       "  'alpha': 0.027230585245890622,\n",
       "  'max_depth': 10,\n",
       "  'colsample_bytree': 0.505661088108428,\n",
       "  'subsample': 0.6226860565996696,\n",
       "  'eta': 0.1485581680577813})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, study = search_boosting(mypipe, X_train, X_test, y_train, y_test)\n",
    "study.best_trial.value, study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 320.85753017665377\n",
      "mse test:  434.98806883468836\n",
      "rmse test:  20.85636758485735\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>calendarYear</th>\n",
       "      <th>fillingDate</th>\n",
       "      <th>real</th>\n",
       "      <th>predicted</th>\n",
       "      <th>previousMarketCap</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>CSCO</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-09-29</td>\n",
       "      <td>640.96695</td>\n",
       "      <td>317.57532</td>\n",
       "      <td>326.03867</td>\n",
       "      <td>-323.39164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2017</td>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>823.17064</td>\n",
       "      <td>534.44330</td>\n",
       "      <td>644.27240</td>\n",
       "      <td>-288.72734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2017</td>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>756.45198</td>\n",
       "      <td>528.85345</td>\n",
       "      <td>578.54152</td>\n",
       "      <td>-227.59853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3066</th>\n",
       "      <td>GE</td>\n",
       "      <td>1997</td>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>389.20414</td>\n",
       "      <td>230.90846</td>\n",
       "      <td>250.48968</td>\n",
       "      <td>-158.29567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>CVX</td>\n",
       "      <td>2016</td>\n",
       "      <td>2017-02-23</td>\n",
       "      <td>238.62691</td>\n",
       "      <td>83.62045</td>\n",
       "      <td>184.75681</td>\n",
       "      <td>-155.00646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>JPM</td>\n",
       "      <td>1999</td>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>94.59199</td>\n",
       "      <td>205.23396</td>\n",
       "      <td>148.31221</td>\n",
       "      <td>110.64198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5717</th>\n",
       "      <td>PFE</td>\n",
       "      <td>2004</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>276.16944</td>\n",
       "      <td>389.14148</td>\n",
       "      <td>391.07994</td>\n",
       "      <td>112.97204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2002</td>\n",
       "      <td>2003-03-11</td>\n",
       "      <td>164.89576</td>\n",
       "      <td>285.91403</td>\n",
       "      <td>321.78971</td>\n",
       "      <td>121.01828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6730</th>\n",
       "      <td>T</td>\n",
       "      <td>1999</td>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>241.21551</td>\n",
       "      <td>419.39914</td>\n",
       "      <td>642.79239</td>\n",
       "      <td>178.18363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>C</td>\n",
       "      <td>2007</td>\n",
       "      <td>2008-02-22</td>\n",
       "      <td>162.99973</td>\n",
       "      <td>357.71085</td>\n",
       "      <td>340.97245</td>\n",
       "      <td>194.71112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1975 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     symbol  calendarYear fillingDate      real  predicted  previousMarketCap  \\\n",
       "1826   CSCO          2000  2000-09-29 640.96695  317.57532          326.03867   \n",
       "3194  GOOGL          2017  2018-02-06 823.17064  534.44330          644.27240   \n",
       "3181   GOOG          2017  2018-02-06 756.45198  528.85345          578.54152   \n",
       "3066     GE          1997  1997-12-31 389.20414  230.90846          250.48968   \n",
       "2003    CVX          2016  2017-02-23 238.62691   83.62045          184.75681   \n",
       "...     ...           ...         ...       ...        ...                ...   \n",
       "3998    JPM          1999  1999-12-31  94.59199  205.23396          148.31221   \n",
       "5717    PFE          2004  2005-02-28 276.16944  389.14148          391.07994   \n",
       "3732   INTC          2002  2003-03-11 164.89576  285.91403          321.78971   \n",
       "6730      T          1999  1999-12-31 241.21551  419.39914          642.79239   \n",
       "1187      C          2007  2008-02-22 162.99973  357.71085          340.97245   \n",
       "\n",
       "          error  \n",
       "1826 -323.39164  \n",
       "3194 -288.72734  \n",
       "3181 -227.59853  \n",
       "3066 -158.29567  \n",
       "2003 -155.00646  \n",
       "...         ...  \n",
       "3998  110.64198  \n",
       "5717  112.97204  \n",
       "3732  121.01828  \n",
       "6730  178.18363  \n",
       "1187  194.71112  \n",
       "\n",
       "[1975 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params =  {\n",
    "    'min_child_weight': 58,\n",
    "    'alpha': 0.027230585245890622,\n",
    "    'max_depth': 10,\n",
    "    'colsample_bytree': 0.505661088108428,\n",
    "    'subsample': 0.6226860565996696,\n",
    "    'eta': 0.1485581680577813\n",
    "  }\n",
    "pipe = Pipeline(steps=[\n",
    "            ('scaler', PowerTransformer()),\n",
    "            ('xgb', XGBRegressor(**params)),\n",
    "])\n",
    "pipe.fit(X_train,y_train)\n",
    "preds_test = pipe.predict(X_test)\n",
    "preds_train = pipe.predict(X_train)\n",
    "mse_test = mean_squared_error(y_true=np.exp(y_test), y_pred=np.exp(preds_test))\n",
    "mse_train = mean_squared_error(y_true=np.exp(y_train), y_pred=np.exp(preds_train))\n",
    "print('mse train:', mse_train)\n",
    "print('mse test: ', mse_test)\n",
    "print('rmse test: ', np.sqrt(mse_test))\n",
    "preds = pipe.predict(X_test)\n",
    "predictions = pd.DataFrame(zip(np.exp(preds), np.exp(y_test)), columns=['predicted', 'real'], index=X_test.index)\n",
    "df_predictions = pd.concat([data.loc[X_test.index,:], predictions], axis=1)\n",
    "df_predictions = df_predictions.sort_values(by = ['symbol', 'calendarYear'])[['symbol','calendarYear','fillingDate','real', 'predicted','previousMarketCap']]\n",
    "df_predictions['error'] = (df_predictions.predicted - df_predictions.real)\n",
    "df_predictions.sort_values(by='error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try on future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = pd.read_csv('../data/processed/future.csv')\n",
    "future_features = future_data.drop(columns=['symbol', 'calendarYear', 'fillingDate', 'target'])\n",
    "future_target = future_data.target\n",
    "preds = pipe.predict(future_features)\n",
    "mse = mean_squared_error(y_true=future_target, y_pred=np.exp(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16340.097082075887"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>calendarYear</th>\n",
       "      <th>fillingDate</th>\n",
       "      <th>real</th>\n",
       "      <th>predicted</th>\n",
       "      <th>previousMarketCap</th>\n",
       "      <th>totalAssets</th>\n",
       "      <th>retainedEarningsToAssets</th>\n",
       "      <th>ebitda</th>\n",
       "      <th>netDebt</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>2522.28195</td>\n",
       "      <td>519.40314</td>\n",
       "      <td>2104.10094</td>\n",
       "      <td>358.01966</td>\n",
       "      <td>0.01585</td>\n",
       "      <td>125.59853</td>\n",
       "      <td>-67.11231</td>\n",
       "      <td>-2002.87881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021-07-29</td>\n",
       "      <td>2232.30744</td>\n",
       "      <td>546.01093</td>\n",
       "      <td>1689.97110</td>\n",
       "      <td>343.44115</td>\n",
       "      <td>0.17094</td>\n",
       "      <td>87.59918</td>\n",
       "      <td>-70.52696</td>\n",
       "      <td>-1686.29652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-10-30</td>\n",
       "      <td>2104.10094</td>\n",
       "      <td>530.19476</td>\n",
       "      <td>1215.20182</td>\n",
       "      <td>342.59619</td>\n",
       "      <td>0.04621</td>\n",
       "      <td>85.70057</td>\n",
       "      <td>-83.97965</td>\n",
       "      <td>-1573.90618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1862.05763</td>\n",
       "      <td>422.60349</td>\n",
       "      <td>1429.52317</td>\n",
       "      <td>361.12663</td>\n",
       "      <td>0.53298</td>\n",
       "      <td>104.05727</td>\n",
       "      <td>-141.53104</td>\n",
       "      <td>-1439.45414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1860.82749</td>\n",
       "      <td>422.60349</td>\n",
       "      <td>1459.94199</td>\n",
       "      <td>361.12663</td>\n",
       "      <td>0.53298</td>\n",
       "      <td>104.05727</td>\n",
       "      <td>-141.53104</td>\n",
       "      <td>-1438.22400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>PYPL</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>162.02229</td>\n",
       "      <td>216.51086</td>\n",
       "      <td>353.19692</td>\n",
       "      <td>76.19595</td>\n",
       "      <td>0.21813</td>\n",
       "      <td>5.62594</td>\n",
       "      <td>-7.28648</td>\n",
       "      <td>54.48858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>BAC</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-22</td>\n",
       "      <td>349.42675</td>\n",
       "      <td>406.86017</td>\n",
       "      <td>328.04066</td>\n",
       "      <td>3185.88421</td>\n",
       "      <td>0.05934</td>\n",
       "      <td>40.82299</td>\n",
       "      <td>-1484.58270</td>\n",
       "      <td>57.43342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2020</td>\n",
       "      <td>2021-01-22</td>\n",
       "      <td>255.79947</td>\n",
       "      <td>325.58777</td>\n",
       "      <td>303.64157</td>\n",
       "      <td>161.42086</td>\n",
       "      <td>0.36732</td>\n",
       "      <td>40.01144</td>\n",
       "      <td>5.44284</td>\n",
       "      <td>69.78830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>CMCSA</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>226.04132</td>\n",
       "      <td>299.44333</td>\n",
       "      <td>249.04645</td>\n",
       "      <td>277.33260</td>\n",
       "      <td>0.22436</td>\n",
       "      <td>37.69791</td>\n",
       "      <td>77.85247</td>\n",
       "      <td>73.40201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>PFE</td>\n",
       "      <td>2019</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>209.02597</td>\n",
       "      <td>294.73920</td>\n",
       "      <td>267.65757</td>\n",
       "      <td>179.21512</td>\n",
       "      <td>0.58314</td>\n",
       "      <td>27.00917</td>\n",
       "      <td>23.72002</td>\n",
       "      <td>85.71323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     symbol  calendarYear fillingDate       real  predicted  \\\n",
       "11     AAPL          2021  2021-10-28 2522.28195  519.40314   \n",
       "945    MSFT          2021  2021-07-29 2232.30744  546.01093   \n",
       "10     AAPL          2020  2020-10-30 2104.10094  530.19476   \n",
       "606   GOOGL          2021  2022-02-02 1862.05763  422.60349   \n",
       "603    GOOG          2021  2022-02-02 1860.82749  422.60349   \n",
       "...     ...           ...         ...        ...        ...   \n",
       "1142   PYPL          2021  2022-02-03  162.02229  216.51086   \n",
       "170     BAC          2021  2022-02-22  349.42675  406.86017   \n",
       "713    INTC          2020  2021-01-22  255.79947  325.58777   \n",
       "293   CMCSA          2021  2022-02-02  226.04132  299.44333   \n",
       "1072    PFE          2019  2020-02-27  209.02597  294.73920   \n",
       "\n",
       "      previousMarketCap  totalAssets  retainedEarningsToAssets    ebitda  \\\n",
       "11           2104.10094    358.01966                   0.01585 125.59853   \n",
       "945          1689.97110    343.44115                   0.17094  87.59918   \n",
       "10           1215.20182    342.59619                   0.04621  85.70057   \n",
       "606          1429.52317    361.12663                   0.53298 104.05727   \n",
       "603          1459.94199    361.12663                   0.53298 104.05727   \n",
       "...                 ...          ...                       ...       ...   \n",
       "1142          353.19692     76.19595                   0.21813   5.62594   \n",
       "170           328.04066   3185.88421                   0.05934  40.82299   \n",
       "713           303.64157    161.42086                   0.36732  40.01144   \n",
       "293           249.04645    277.33260                   0.22436  37.69791   \n",
       "1072          267.65757    179.21512                   0.58314  27.00917   \n",
       "\n",
       "         netDebt       error  \n",
       "11     -67.11231 -2002.87881  \n",
       "945    -70.52696 -1686.29652  \n",
       "10     -83.97965 -1573.90618  \n",
       "606   -141.53104 -1439.45414  \n",
       "603   -141.53104 -1438.22400  \n",
       "...          ...         ...  \n",
       "1142    -7.28648    54.48858  \n",
       "170  -1484.58270    57.43342  \n",
       "713      5.44284    69.78830  \n",
       "293     77.85247    73.40201  \n",
       "1072    23.72002    85.71323  \n",
       "\n",
       "[1494 rows x 11 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(zip(np.exp(preds), future_target), columns=['predicted', 'real'], index=future_features.index)\n",
    "df_predictions = pd.concat([future_data.loc[future_features.index,:], predictions], axis=1)\n",
    "context = ['symbol','calendarYear','fillingDate','real', 'predicted','previousMarketCap', 'totalAssets', 'retainedEarningsToAssets', 'ebitda', 'netDebt']\n",
    "df_predictions = df_predictions.sort_values(by = ['symbol', 'calendarYear']).loc[:,context]\n",
    "df_predictions['error'] = (df_predictions.predicted - df_predictions.real)\n",
    "df_predictions.sort_values(by='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions.predicted.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>previousMarketCap</td>\n",
       "      <td>0.38263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ebitda</td>\n",
       "      <td>0.12965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>revenue</td>\n",
       "      <td>0.07116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>totalAssets</td>\n",
       "      <td>0.03164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>researchAndDevelopmentExpensesToRevenue</td>\n",
       "      <td>0.02967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>operatingIncomeToRevenue</td>\n",
       "      <td>0.02200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>stockBasedCompensationToRevenue</td>\n",
       "      <td>0.01793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shortTermInvestmentsToAssets</td>\n",
       "      <td>0.01544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>netIncomeToRevenue</td>\n",
       "      <td>0.01173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>freeCashFlowToRevenue</td>\n",
       "      <td>0.01134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>roe</td>\n",
       "      <td>0.01046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>accountPayablesToAssets</td>\n",
       "      <td>0.00812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>purchasesOfInvestmentsToRevenue</td>\n",
       "      <td>0.00810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>netDebt</td>\n",
       "      <td>0.00778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>dividendsPaidToRevenue</td>\n",
       "      <td>0.00776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>costOfRevenueToRevenue</td>\n",
       "      <td>0.00717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>changeInWorkingCapitalToRevenueYearOverYear</td>\n",
       "      <td>0.00702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>otherNonCurrentLiabilitiesToAssets</td>\n",
       "      <td>0.00701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>totalStockholdersEquityToAssets</td>\n",
       "      <td>0.00696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>otherNonCurrentAssetsToAssets</td>\n",
       "      <td>0.00693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>longTermDebtToAssets</td>\n",
       "      <td>0.00673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>revenueYoY</td>\n",
       "      <td>0.00643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>netReceivablesToAssets</td>\n",
       "      <td>0.00614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>intangibleAssetsToAssets</td>\n",
       "      <td>0.00608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ebitdaToRevenueYearOverYear</td>\n",
       "      <td>0.00594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>netSharesRepurchasedToRevenue</td>\n",
       "      <td>0.00557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>capitalExpenditureToRevenueYearOverYear</td>\n",
       "      <td>0.00557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>changeInWorkingCapitalToRevenue</td>\n",
       "      <td>0.00556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>retainedEarningsToAssets</td>\n",
       "      <td>0.00550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>netDebtToAssets</td>\n",
       "      <td>0.00534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sellingGeneralAndAdministrativeExpensesToRevenue</td>\n",
       "      <td>0.00530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>researchAndDevelopmentExpensesToRevenueYearOve...</td>\n",
       "      <td>0.00509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>depreciationAndAmortizationToRevenue</td>\n",
       "      <td>0.00508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>freeCashFlowToRevenueYearOverYear</td>\n",
       "      <td>0.00503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>shortTermDebtToAssets</td>\n",
       "      <td>0.00502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>costOfRevenueToRevenueYearOverYear</td>\n",
       "      <td>0.00500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>netIncomeToRevenueYearOverYear</td>\n",
       "      <td>0.00493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>operatingIncomeToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cashAndCashEquivalentsToAssets</td>\n",
       "      <td>0.00475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>netDebtToEBITDA</td>\n",
       "      <td>0.00475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ebitdaToRevenue</td>\n",
       "      <td>0.00467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>freeCashFlowGivenToShareholders</td>\n",
       "      <td>0.00452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>otherCurrentAssetsToAssets</td>\n",
       "      <td>0.00441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>revenueYoYSMA3</td>\n",
       "      <td>0.00434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>PPEtoSales</td>\n",
       "      <td>0.00431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>deferredRevenueToAssets</td>\n",
       "      <td>0.00429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>otherCurrentLiabilitiesToAssets</td>\n",
       "      <td>0.00418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>capitalExpenditureToRevenue</td>\n",
       "      <td>0.00413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ebitdaToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sellingGeneralAndAdministrativeExpensesToReven...</td>\n",
       "      <td>0.00409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>netIncomeToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>inventoryToAssets</td>\n",
       "      <td>0.00400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>longTermInvestmentsToAssets</td>\n",
       "      <td>0.00379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>stockBasedCompensationToRevenueYearOverYear</td>\n",
       "      <td>0.00369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>dividendsPaidToRevenueYearOverYear</td>\n",
       "      <td>0.00367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>operatingIncomeToRevenueYearOverYear</td>\n",
       "      <td>0.00362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sellingGeneralAndAdministrativeExpensesToReven...</td>\n",
       "      <td>0.00360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>acquisitionsNetToRevenueYearOverYear</td>\n",
       "      <td>0.00353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>depreciationAndAmortizationToRevenueYearOverYear</td>\n",
       "      <td>0.00352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>costOfRevenueToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>acquisitionsNetToRevenue</td>\n",
       "      <td>0.00290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>propertyPlantEquipmentNetToAssets</td>\n",
       "      <td>0.00283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>researchAndDevelopmentExpensesToRevenueYearOve...</td>\n",
       "      <td>0.00270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>purchasesOfInvestmentsToRevenueYearOverYear</td>\n",
       "      <td>0.00252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0       1\n",
       "6                                   previousMarketCap 0.38263\n",
       "3                                              ebitda 0.12965\n",
       "1                                             revenue 0.07116\n",
       "0                                         totalAssets 0.03164\n",
       "26            researchAndDevelopmentExpensesToRevenue 0.02967\n",
       "29                           operatingIncomeToRevenue 0.02200\n",
       "43                    stockBasedCompensationToRevenue 0.01793\n",
       "8                        shortTermInvestmentsToAssets 0.01544\n",
       "30                                 netIncomeToRevenue 0.01173\n",
       "46                              freeCashFlowToRevenue 0.01134\n",
       "63                                                roe 0.01046\n",
       "16                            accountPayablesToAssets 0.00812\n",
       "49                    purchasesOfInvestmentsToRevenue 0.00810\n",
       "2                                             netDebt 0.00778\n",
       "50                             dividendsPaidToRevenue 0.00776\n",
       "25                             costOfRevenueToRevenue 0.00717\n",
       "54        changeInWorkingCapitalToRevenueYearOverYear 0.00702\n",
       "21                 otherNonCurrentLiabilitiesToAssets 0.00701\n",
       "23                    totalStockholdersEquityToAssets 0.00696\n",
       "15                      otherNonCurrentAssetsToAssets 0.00693\n",
       "20                               longTermDebtToAssets 0.00673\n",
       "4                                          revenueYoY 0.00643\n",
       "9                              netReceivablesToAssets 0.00614\n",
       "13                           intangibleAssetsToAssets 0.00608\n",
       "34                        ebitdaToRevenueYearOverYear 0.00594\n",
       "51                      netSharesRepurchasedToRevenue 0.00557\n",
       "56            capitalExpenditureToRevenueYearOverYear 0.00557\n",
       "45                    changeInWorkingCapitalToRevenue 0.00556\n",
       "22                           retainedEarningsToAssets 0.00550\n",
       "24                                    netDebtToAssets 0.00534\n",
       "27   sellingGeneralAndAdministrativeExpensesToRevenue 0.00530\n",
       "38  researchAndDevelopmentExpensesToRevenueYearOve... 0.00509\n",
       "44               depreciationAndAmortizationToRevenue 0.00508\n",
       "55                  freeCashFlowToRevenueYearOverYear 0.00503\n",
       "17                              shortTermDebtToAssets 0.00502\n",
       "31                 costOfRevenueToRevenueYearOverYear 0.00500\n",
       "36                     netIncomeToRevenueYearOverYear 0.00493\n",
       "41           operatingIncomeToRevenueYearOverYearSMA3 0.00488\n",
       "7                      cashAndCashEquivalentsToAssets 0.00475\n",
       "62                                    netDebtToEBITDA 0.00475\n",
       "28                                    ebitdaToRevenue 0.00467\n",
       "60                    freeCashFlowGivenToShareholders 0.00452\n",
       "11                         otherCurrentAssetsToAssets 0.00441\n",
       "5                                      revenueYoYSMA3 0.00434\n",
       "61                                         PPEtoSales 0.00431\n",
       "18                            deferredRevenueToAssets 0.00429\n",
       "19                    otherCurrentLiabilitiesToAssets 0.00418\n",
       "47                        capitalExpenditureToRevenue 0.00413\n",
       "40                    ebitdaToRevenueYearOverYearSMA3 0.00409\n",
       "33  sellingGeneralAndAdministrativeExpensesToReven... 0.00409\n",
       "42                 netIncomeToRevenueYearOverYearSMA3 0.00404\n",
       "10                                  inventoryToAssets 0.00400\n",
       "14                        longTermInvestmentsToAssets 0.00379\n",
       "52        stockBasedCompensationToRevenueYearOverYear 0.00369\n",
       "59                 dividendsPaidToRevenueYearOverYear 0.00367\n",
       "35               operatingIncomeToRevenueYearOverYear 0.00362\n",
       "39  sellingGeneralAndAdministrativeExpensesToReven... 0.00360\n",
       "57               acquisitionsNetToRevenueYearOverYear 0.00353\n",
       "53   depreciationAndAmortizationToRevenueYearOverYear 0.00352\n",
       "37             costOfRevenueToRevenueYearOverYearSMA3 0.00333\n",
       "48                           acquisitionsNetToRevenue 0.00290\n",
       "12                  propertyPlantEquipmentNetToAssets 0.00283\n",
       "32  researchAndDevelopmentExpensesToRevenueYearOve... 0.00270\n",
       "58        purchasesOfInvestmentsToRevenueYearOverYear 0.00252"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(future_features.columns, pipe['xgb'].feature_importances_)).sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF... Inspect these values\n",
    "- Awful at predicting tech which is causing the huge mae\n",
    "- Maybe it has to do with the fact the its capturing the trends of the times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans\n",
    "- Inconclusive Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "clusters = []\n",
    "inertias = []\n",
    "ch_scores = []\n",
    "sil_scores = []\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps = [\n",
    "        ('feature_clipper', Windsorizer()),\n",
    "        ('normalizer', PowerTransformer())\n",
    "    ]\n",
    ")\n",
    "data = pipe.fit_transform(X_train)\n",
    "for a in range(2,20):\n",
    "    print(a)\n",
    "    kmeans = KMeans(n_clusters=a)\n",
    "    kmeans.fit(data)\n",
    "    clusters.append(a)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    ch_scores.append(calinski_harabasz_score(data, kmeans.labels_))\n",
    "    sil_scores.append(silhouette_score(data, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(clusters, inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertias\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(clusters, sil_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(clusters, ch_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"CH score\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> This does not do what I want, I want to transform each observation to de distance to its centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2\n",
    "- clipping the the previousMarketCap feature destroys outliers. \n",
    "  - Filter clipping by kurtosis\n",
    "  - Predict Increases\n",
    "- Try to simulate a neural network with different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to predict separating train on test based on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1435, 146), (4883, 146))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/processed/present.csv')\n",
    "data.query(\"calendarYear >= 2016\").shape, data.query(\"calendarYear < 2016\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = data.query(\"calendarYear > 2016\").drop(columns=['target', 'symbol', 'calendarYear', 'fillingDate'])\n",
    "target_test = np.log(data.query(\"calendarYear > 2016\").target)\n",
    "features_train = data.query(\"calendarYear <= 2016\").drop(columns=['target', 'symbol', 'calendarYear', 'fillingDate'])\n",
    "target_train = np.log(data.query(\"calendarYear <= 2016\").target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5351, 141), (967, 141), (5351,), (967,)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in [features_train, features_test, target_train, target_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 1190.047664523396\n",
      "mse test:  3253.006347114777\n",
      "rmse test:  57.0351325685737\n"
     ]
    }
   ],
   "source": [
    "mypipe = Pipeline(steps=[\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ])\n",
    "do_linear_regression(mypipe, features_train, features_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At least we've managed to replicate the problem, now we have something we can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-20 00:48:09,179]\u001b[0m A new study created in memory with name: no-name-724ff54e-b55f-4984-9d5b-329e0394da76\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:13,083]\u001b[0m Trial 0 finished with value: 6827.503915112516 and parameters: {'min_child_weight': 354, 'alpha': 4.453202509992071e-05, 'max_depth': 8, 'colsample_bytree': 0.9664625408466313, 'subsample': 0.5397659499403213, 'eta': 0.036738173411862765}. Best is trial 0 with value: 6827.503915112516.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:16,695]\u001b[0m Trial 1 finished with value: 8244.479708284107 and parameters: {'min_child_weight': 454, 'alpha': 0.05020610339937164, 'max_depth': 10, 'colsample_bytree': 0.625442697077506, 'subsample': 0.975553451965957, 'eta': 0.021579076684322122}. Best is trial 0 with value: 6827.503915112516.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:20,303]\u001b[0m Trial 2 finished with value: 4182.504190722922 and parameters: {'min_child_weight': 243, 'alpha': 1.0611792541174954e-05, 'max_depth': 12, 'colsample_bytree': 0.6470989022353311, 'subsample': 0.5834123058425151, 'eta': 0.1884800374684717}. Best is trial 2 with value: 4182.504190722922.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:25,373]\u001b[0m Trial 3 finished with value: 3367.2936460525634 and parameters: {'min_child_weight': 216, 'alpha': 7.838617333796165e-06, 'max_depth': 11, 'colsample_bytree': 0.7353671347582642, 'subsample': 0.9020278100939726, 'eta': 0.14697754694034007}. Best is trial 3 with value: 3367.2936460525634.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:29,984]\u001b[0m Trial 4 finished with value: 10514.417247717416 and parameters: {'min_child_weight': 184, 'alpha': 0.00013011298462666494, 'max_depth': 9, 'colsample_bytree': 0.8971001248797501, 'subsample': 0.923787467509354, 'eta': 0.011888272167074887}. Best is trial 3 with value: 3367.2936460525634.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:35,554]\u001b[0m Trial 5 finished with value: 5578.9928909170885 and parameters: {'min_child_weight': 50, 'alpha': 0.4764282980868004, 'max_depth': 11, 'colsample_bytree': 0.818848000280486, 'subsample': 0.915833722284541, 'eta': 0.022920215175427847}. Best is trial 3 with value: 3367.2936460525634.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:39,048]\u001b[0m Trial 6 finished with value: 10976.763642841825 and parameters: {'min_child_weight': 343, 'alpha': 0.008187194329038064, 'max_depth': 10, 'colsample_bytree': 0.8357121135754817, 'subsample': 0.6625481845155159, 'eta': 0.012130402289422677}. Best is trial 3 with value: 3367.2936460525634.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:42,412]\u001b[0m Trial 7 finished with value: 9898.043613932086 and parameters: {'min_child_weight': 317, 'alpha': 0.0002388146193366383, 'max_depth': 6, 'colsample_bytree': 0.7147579783489368, 'subsample': 0.6380272218311505, 'eta': 0.015569034376215683}. Best is trial 3 with value: 3367.2936460525634.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:46,091]\u001b[0m Trial 8 finished with value: 8185.918429215331 and parameters: {'min_child_weight': 160, 'alpha': 0.0022120278780971224, 'max_depth': 6, 'colsample_bytree': 0.822977857222327, 'subsample': 0.5425331481987223, 'eta': 0.019016296536397567}. Best is trial 3 with value: 3367.2936460525634.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:49,526]\u001b[0m Trial 9 finished with value: 4430.661759278746 and parameters: {'min_child_weight': 160, 'alpha': 0.000470814943629654, 'max_depth': 6, 'colsample_bytree': 0.4498090320827861, 'subsample': 0.5063629275850063, 'eta': 0.057076001485801275}. Best is trial 3 with value: 3367.2936460525634.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:48:55,790]\u001b[0m Trial 10 finished with value: 1729.811387845955 and parameters: {'min_child_weight': 35, 'alpha': 1.2103311603954833e-06, 'max_depth': 15, 'colsample_bytree': 0.49182598177465187, 'subsample': 0.7834388025663374, 'eta': 0.16044319041885222}. Best is trial 10 with value: 1729.811387845955.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:01,363]\u001b[0m Trial 11 finished with value: 2115.391093103373 and parameters: {'min_child_weight': 65, 'alpha': 1.0813469321542554e-06, 'max_depth': 15, 'colsample_bytree': 0.40473230726024656, 'subsample': 0.8113182422809152, 'eta': 0.18716285296729182}. Best is trial 10 with value: 1729.811387845955.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:07,525]\u001b[0m Trial 12 finished with value: 1719.6855118686476 and parameters: {'min_child_weight': 31, 'alpha': 1.361929702294554e-06, 'max_depth': 15, 'colsample_bytree': 0.4038606672372587, 'subsample': 0.795265863870691, 'eta': 0.10829704398185656}. Best is trial 12 with value: 1719.6855118686476.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:14,363]\u001b[0m Trial 13 finished with value: 1887.2857725739161 and parameters: {'min_child_weight': 22, 'alpha': 1.0140022201853968e-06, 'max_depth': 15, 'colsample_bytree': 0.5105272259527196, 'subsample': 0.7725012096896856, 'eta': 0.09553302425326711}. Best is trial 12 with value: 1719.6855118686476.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:19,355]\u001b[0m Trial 14 finished with value: 2687.99203837266 and parameters: {'min_child_weight': 95, 'alpha': 8.801466990083862e-06, 'max_depth': 13, 'colsample_bytree': 0.5226140468723971, 'subsample': 0.816144263929714, 'eta': 0.09783365196800912}. Best is trial 12 with value: 1719.6855118686476.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:23,793]\u001b[0m Trial 15 finished with value: 2884.248775721628 and parameters: {'min_child_weight': 109, 'alpha': 3.290227670541e-06, 'max_depth': 14, 'colsample_bytree': 0.5249503797418076, 'subsample': 0.7045347240416039, 'eta': 0.09323437105269915}. Best is trial 12 with value: 1719.6855118686476.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:28,341]\u001b[0m Trial 16 finished with value: 3206.5036148157733 and parameters: {'min_child_weight': 107, 'alpha': 4.105848068771812e-05, 'max_depth': 13, 'colsample_bytree': 0.5892527778598904, 'subsample': 0.7394710004090237, 'eta': 0.05957631015621495}. Best is trial 12 with value: 1719.6855118686476.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:31,561]\u001b[0m Trial 17 finished with value: 5141.613999021495 and parameters: {'min_child_weight': 475, 'alpha': 3.77214443361397e-05, 'max_depth': 14, 'colsample_bytree': 0.40302262295074515, 'subsample': 0.847677383717266, 'eta': 0.11202363850618641}. Best is trial 12 with value: 1719.6855118686476.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:37,056]\u001b[0m Trial 18 finished with value: 2022.6160057986785 and parameters: {'min_child_weight': 26, 'alpha': 0.002036521028047286, 'max_depth': 15, 'colsample_bytree': 0.46325205852574763, 'subsample': 0.7284921790990814, 'eta': 0.07377480584519992}. Best is trial 12 with value: 1719.6855118686476.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:40,819]\u001b[0m Trial 19 finished with value: 5727.137240097225 and parameters: {'min_child_weight': 302, 'alpha': 2.9278488051429775e-06, 'max_depth': 13, 'colsample_bytree': 0.5752153881216661, 'subsample': 0.8591739618836522, 'eta': 0.03522239455852789}. Best is trial 12 with value: 1719.6855118686476.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:45,327]\u001b[0m Trial 20 finished with value: 2802.4646113823123 and parameters: {'min_child_weight': 125, 'alpha': 0.02714800012487897, 'max_depth': 14, 'colsample_bytree': 0.4721314037845487, 'subsample': 0.7782348785380572, 'eta': 0.14284727932375796}. Best is trial 12 with value: 1719.6855118686476.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:51,442]\u001b[0m Trial 21 finished with value: 1563.4595677001196 and parameters: {'min_child_weight': 22, 'alpha': 1.5265888790593132e-06, 'max_depth': 15, 'colsample_bytree': 0.5305963423880082, 'subsample': 0.7801966784342731, 'eta': 0.12195741197435278}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:49:56,821]\u001b[0m Trial 22 finished with value: 2406.4049726442067 and parameters: {'min_child_weight': 69, 'alpha': 3.597139398222401e-06, 'max_depth': 15, 'colsample_bytree': 0.5672846021013558, 'subsample': 0.6871402133717488, 'eta': 0.13435706566891548}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:02,158]\u001b[0m Trial 23 finished with value: 2044.6195299184728 and parameters: {'min_child_weight': 30, 'alpha': 2.1647806376510685e-05, 'max_depth': 14, 'colsample_bytree': 0.45136423654599556, 'subsample': 0.8037323315265994, 'eta': 0.07682670573925272}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:08,142]\u001b[0m Trial 24 finished with value: 2266.9509329591633 and parameters: {'min_child_weight': 75, 'alpha': 1.301895881913683e-06, 'max_depth': 12, 'colsample_bytree': 0.6614903600591839, 'subsample': 0.8698237962183277, 'eta': 0.12571417768621482}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:12,736]\u001b[0m Trial 25 finished with value: 3139.7378303387086 and parameters: {'min_child_weight': 145, 'alpha': 3.928271040888958e-06, 'max_depth': 15, 'colsample_bytree': 0.4886099949461456, 'subsample': 0.7467458048418065, 'eta': 0.16959462457654145}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:15,708]\u001b[0m Trial 26 finished with value: 6093.3166270173815 and parameters: {'min_child_weight': 408, 'alpha': 0.0001312098747353243, 'max_depth': 12, 'colsample_bytree': 0.4051518979911143, 'subsample': 0.6132975465908772, 'eta': 0.05858751914280921}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:19,952]\u001b[0m Trial 27 finished with value: 3824.4518077596367 and parameters: {'min_child_weight': 209, 'alpha': 1.4308882824850979e-05, 'max_depth': 13, 'colsample_bytree': 0.5752469595071598, 'subsample': 0.7748081328941134, 'eta': 0.0783761784493059}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:25,353]\u001b[0m Trial 28 finished with value: 2354.8808626296454 and parameters: {'min_child_weight': 78, 'alpha': 2.1810046751190404e-06, 'max_depth': 14, 'colsample_bytree': 0.5299021173064322, 'subsample': 0.828594725725806, 'eta': 0.11671442142699484}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:28,537]\u001b[0m Trial 29 finished with value: 5313.674568152486 and parameters: {'min_child_weight': 278, 'alpha': 4.2349811012918515e-05, 'max_depth': 5, 'colsample_bytree': 0.4465858328340185, 'subsample': 0.7105500745042453, 'eta': 0.04419526989690548}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:34,347]\u001b[0m Trial 30 finished with value: 2952.8518513016797 and parameters: {'min_child_weight': 20, 'alpha': 0.758630163965161, 'max_depth': 9, 'colsample_bytree': 0.7529043662340379, 'subsample': 0.9653602434772391, 'eta': 0.03527790893340645}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:39,715]\u001b[0m Trial 31 finished with value: 2193.18737743469 and parameters: {'min_child_weight': 45, 'alpha': 1.0014640876541577e-06, 'max_depth': 15, 'colsample_bytree': 0.49667559141682, 'subsample': 0.7755468859325755, 'eta': 0.09973164801977297}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:46,276]\u001b[0m Trial 32 finished with value: 1632.842850601681 and parameters: {'min_child_weight': 20, 'alpha': 4.277590783701931e-06, 'max_depth': 15, 'colsample_bytree': 0.5402801612644051, 'subsample': 0.7730127598202254, 'eta': 0.15190261648623085}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:53,055]\u001b[0m Trial 33 finished with value: 2772.055529766611 and parameters: {'min_child_weight': 129, 'alpha': 5.87600664162406e-06, 'max_depth': 14, 'colsample_bytree': 0.989219197265373, 'subsample': 0.795428989957575, 'eta': 0.15134705410298427}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:50:59,619]\u001b[0m Trial 34 finished with value: 2014.4760128618507 and parameters: {'min_child_weight': 87, 'alpha': 1.1794859517506731e-05, 'max_depth': 15, 'colsample_bytree': 0.644789885610703, 'subsample': 0.8887316979951866, 'eta': 0.1921718146953474}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:05,782]\u001b[0m Trial 35 finished with value: 2072.5683492220737 and parameters: {'min_child_weight': 52, 'alpha': 2.0692456009730387e-06, 'max_depth': 13, 'colsample_bytree': 0.6144919743957967, 'subsample': 0.8423518276916835, 'eta': 0.161658541896273}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:09,535]\u001b[0m Trial 36 finished with value: 4009.968019328926 and parameters: {'min_child_weight': 406, 'alpha': 1.8788932747729762e-05, 'max_depth': 11, 'colsample_bytree': 0.5470273542802656, 'subsample': 0.9467445328496356, 'eta': 0.19783489758875694}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:14,900]\u001b[0m Trial 37 finished with value: 2353.8587638592126 and parameters: {'min_child_weight': 51, 'alpha': 6.205021361351492e-06, 'max_depth': 14, 'colsample_bytree': 0.6077123849982, 'subsample': 0.6674247656600844, 'eta': 0.1188971456064138}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:19,338]\u001b[0m Trial 38 finished with value: 3635.3555102284 and parameters: {'min_child_weight': 235, 'alpha': 2.001999864947699e-06, 'max_depth': 12, 'colsample_bytree': 0.4363563129710389, 'subsample': 0.7520619346606494, 'eta': 0.14046588573574345}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:23,792]\u001b[0m Trial 39 finished with value: 3282.8846095488884 and parameters: {'min_child_weight': 180, 'alpha': 7.883305355914087e-05, 'max_depth': 8, 'colsample_bytree': 0.6654845588378132, 'subsample': 0.8854206912698915, 'eta': 0.163180688003277}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:28,759]\u001b[0m Trial 40 finished with value: 2280.4583711823516 and parameters: {'min_child_weight': 49, 'alpha': 0.20199054488917192, 'max_depth': 15, 'colsample_bytree': 0.48683049497289804, 'subsample': 0.7240441757891596, 'eta': 0.08354139041347376}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:34,658]\u001b[0m Trial 41 finished with value: 1852.803455226392 and parameters: {'min_child_weight': 23, 'alpha': 1.0826826943123742e-06, 'max_depth': 15, 'colsample_bytree': 0.4932908458887926, 'subsample': 0.7665214263572083, 'eta': 0.10705989140390486}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:40,606]\u001b[0m Trial 42 finished with value: 1935.0072193678195 and parameters: {'min_child_weight': 42, 'alpha': 5.529052987144764e-06, 'max_depth': 15, 'colsample_bytree': 0.5462406756739653, 'subsample': 0.7577869712781122, 'eta': 0.11138625894550454}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:45,861]\u001b[0m Trial 43 finished with value: 4443.146914177783 and parameters: {'min_child_weight': 20, 'alpha': 1.9207839187911603e-06, 'max_depth': 14, 'colsample_bytree': 0.4192955452465268, 'subsample': 0.8247273709339094, 'eta': 0.028124749095621092}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:51,015]\u001b[0m Trial 44 finished with value: 2583.2875103303113 and parameters: {'min_child_weight': 92, 'alpha': 7.412943932018491e-06, 'max_depth': 15, 'colsample_bytree': 0.49677865342636224, 'subsample': 0.7929472643211004, 'eta': 0.1343948103144601}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:51:57,196]\u001b[0m Trial 45 finished with value: 2378.991163477252 and parameters: {'min_child_weight': 64, 'alpha': 1.6405792795272476e-06, 'max_depth': 14, 'colsample_bytree': 0.9096752437512585, 'subsample': 0.6922355239894955, 'eta': 0.06456890714050668}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:01,574]\u001b[0m Trial 46 finished with value: 2567.6161644269164 and parameters: {'min_child_weight': 113, 'alpha': 3.720222628427055e-06, 'max_depth': 13, 'colsample_bytree': 0.4297589221518867, 'subsample': 0.7608255029623164, 'eta': 0.1720027575492649}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:06,233]\u001b[0m Trial 47 finished with value: 2474.7170457172256 and parameters: {'min_child_weight': 82, 'alpha': 1.8095404379955857e-05, 'max_depth': 10, 'colsample_bytree': 0.5491131977057446, 'subsample': 0.7987657926213182, 'eta': 0.08863581439716349}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:11,152]\u001b[0m Trial 48 finished with value: 2628.8145932119655 and parameters: {'min_child_weight': 41, 'alpha': 0.0002881045983627203, 'max_depth': 15, 'colsample_bytree': 0.5102176489821533, 'subsample': 0.7237300233645946, 'eta': 0.048856159538402444}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:16,144]\u001b[0m Trial 49 finished with value: 2049.561915298024 and parameters: {'min_child_weight': 64, 'alpha': 1.0511581001617173e-06, 'max_depth': 14, 'colsample_bytree': 0.4605723463356932, 'subsample': 0.8253268800212935, 'eta': 0.10687629697694973}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:21,681]\u001b[0m Trial 50 finished with value: 2664.5317641957686 and parameters: {'min_child_weight': 101, 'alpha': 9.413917020975386e-06, 'max_depth': 15, 'colsample_bytree': 0.7077996258920369, 'subsample': 0.8465097831607434, 'eta': 0.06941266389332931}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:27,269]\u001b[0m Trial 51 finished with value: 2061.5830501065398 and parameters: {'min_child_weight': 42, 'alpha': 2.933562467017962e-06, 'max_depth': 15, 'colsample_bytree': 0.5323966013296624, 'subsample': 0.7752594187997147, 'eta': 0.09553194797295375}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:32,742]\u001b[0m Trial 52 finished with value: 1772.5581155383027 and parameters: {'min_child_weight': 24, 'alpha': 1.2812501639052432e-06, 'max_depth': 14, 'colsample_bytree': 0.4763180846134293, 'subsample': 0.740486196604071, 'eta': 0.12572725334248147}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:38,070]\u001b[0m Trial 53 finished with value: 1864.9965813382496 and parameters: {'min_child_weight': 23, 'alpha': 4.7281117704499446e-06, 'max_depth': 14, 'colsample_bytree': 0.47600827300224, 'subsample': 0.6502149298866086, 'eta': 0.12563921855003024}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:43,086]\u001b[0m Trial 54 finished with value: 2186.4755053965823 and parameters: {'min_child_weight': 63, 'alpha': 1.558329453683052e-06, 'max_depth': 13, 'colsample_bytree': 0.5070549748276929, 'subsample': 0.7362747166662855, 'eta': 0.14763430578195222}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:49,861]\u001b[0m Trial 55 finished with value: 1650.496908937122 and parameters: {'min_child_weight': 33, 'alpha': 0.0007511835572062458, 'max_depth': 15, 'colsample_bytree': 0.7783599107576104, 'subsample': 0.6784326752675145, 'eta': 0.17757842152300987}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:52:54,646]\u001b[0m Trial 56 finished with value: 3165.9243789499556 and parameters: {'min_child_weight': 126, 'alpha': 0.0014534790564203544, 'max_depth': 14, 'colsample_bytree': 0.7639722159207953, 'subsample': 0.6038988889050532, 'eta': 0.17482167953802583}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:01,105]\u001b[0m Trial 57 finished with value: 2419.098766481111 and parameters: {'min_child_weight': 60, 'alpha': 0.004825905684744083, 'max_depth': 15, 'colsample_bytree': 0.8328047787797779, 'subsample': 0.6884664571918855, 'eta': 0.1296037250783088}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:06,975]\u001b[0m Trial 58 finished with value: 2010.6483616943663 and parameters: {'min_child_weight': 37, 'alpha': 0.02872155636960492, 'max_depth': 11, 'colsample_bytree': 0.7896529806683898, 'subsample': 0.6648520931866125, 'eta': 0.15382192470654574}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:11,495]\u001b[0m Trial 59 finished with value: 3275.303493744866 and parameters: {'min_child_weight': 156, 'alpha': 0.09622068350034547, 'max_depth': 7, 'colsample_bytree': 0.8722129179108267, 'subsample': 0.707183884996701, 'eta': 0.16751087752497568}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:14,984]\u001b[0m Trial 60 finished with value: 11646.919215757362 and parameters: {'min_child_weight': 363, 'alpha': 0.0005495791714580481, 'max_depth': 14, 'colsample_bytree': 0.6879220415038528, 'subsample': 0.6304245830150473, 'eta': 0.010471859288288915}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:21,167]\u001b[0m Trial 61 finished with value: 1908.7934862774362 and parameters: {'min_child_weight': 35, 'alpha': 2.6006513873832713e-06, 'max_depth': 15, 'colsample_bytree': 0.5930268714322954, 'subsample': 0.7906627359822591, 'eta': 0.10815311832661982}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:25,495]\u001b[0m Trial 62 finished with value: 2844.864660556068 and parameters: {'min_child_weight': 80, 'alpha': 1.0172168118424703e-06, 'max_depth': 15, 'colsample_bytree': 0.468831677655133, 'subsample': 0.5641032407564585, 'eta': 0.18130915209635937}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:33,233]\u001b[0m Trial 63 finished with value: 1683.4802853867975 and parameters: {'min_child_weight': 22, 'alpha': 2.583455022473297e-05, 'max_depth': 15, 'colsample_bytree': 0.7983139471183226, 'subsample': 0.8119246784167673, 'eta': 0.14126404183700522}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:40,251]\u001b[0m Trial 64 finished with value: 2348.9084122259737 and parameters: {'min_child_weight': 55, 'alpha': 2.778743362716247e-05, 'max_depth': 14, 'colsample_bytree': 0.8013178051247315, 'subsample': 0.8705060083064048, 'eta': 0.14092018186609725}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:47,476]\u001b[0m Trial 65 finished with value: 1716.384574034881 and parameters: {'min_child_weight': 35, 'alpha': 7.37112010196082e-05, 'max_depth': 13, 'colsample_bytree': 0.8560124157967585, 'subsample': 0.8158582672394062, 'eta': 0.19916036098539497}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:53:53,748]\u001b[0m Trial 66 finished with value: 2290.2923030014954 and parameters: {'min_child_weight': 103, 'alpha': 6.789927647074253e-05, 'max_depth': 13, 'colsample_bytree': 0.8579702743949548, 'subsample': 0.8143130483675995, 'eta': 0.19611288977446406}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:54:00,245]\u001b[0m Trial 67 finished with value: 2087.980086594556 and parameters: {'min_child_weight': 71, 'alpha': 0.00016625604075738904, 'max_depth': 15, 'colsample_bytree': 0.7355672108874477, 'subsample': 0.8356015615802236, 'eta': 0.15238950537877902}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:54:05,749]\u001b[0m Trial 68 finished with value: 7874.447072733767 and parameters: {'min_child_weight': 36, 'alpha': 0.0005211059435285758, 'max_depth': 13, 'colsample_bytree': 0.816959560799266, 'subsample': 0.8077645660604559, 'eta': 0.016835162982597594}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:54:12,357]\u001b[0m Trial 69 finished with value: 2094.1995266457748 and parameters: {'min_child_weight': 87, 'alpha': 0.005074755927601012, 'max_depth': 12, 'colsample_bytree': 0.8862360768108807, 'subsample': 0.8568231769533937, 'eta': 0.18027248746164848}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:54:18,054]\u001b[0m Trial 70 finished with value: 2805.620631373774 and parameters: {'min_child_weight': 116, 'alpha': 8.1156379699073e-05, 'max_depth': 10, 'colsample_bytree': 0.9074354950078738, 'subsample': 0.7917489899055518, 'eta': 0.15783845303149094}. Best is trial 21 with value: 1563.4595677001196.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:54:25,506]\u001b[0m Trial 71 finished with value: 1378.5020486827655 and parameters: {'min_child_weight': 20, 'alpha': 1.2543600568789481e-05, 'max_depth': 14, 'colsample_bytree': 0.8493133766064039, 'subsample': 0.7445368987614208, 'eta': 0.1205462373176862}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:54:32,082]\u001b[0m Trial 72 finished with value: 1962.8380349398542 and parameters: {'min_child_weight': 53, 'alpha': 1.2157754332367558e-05, 'max_depth': 15, 'colsample_bytree': 0.7596763700713242, 'subsample': 0.7780892363773241, 'eta': 0.1194708778991596}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:54:40,268]\u001b[0m Trial 73 finished with value: 1539.5427212400168 and parameters: {'min_child_weight': 32, 'alpha': 3.147529924306753e-05, 'max_depth': 15, 'colsample_bytree': 0.9440023975438425, 'subsample': 0.8135945107682727, 'eta': 0.13983226460232165}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:54:47,675]\u001b[0m Trial 74 finished with value: 2290.4648289419965 and parameters: {'min_child_weight': 70, 'alpha': 5.440976159259521e-05, 'max_depth': 14, 'colsample_bytree': 0.973646500408875, 'subsample': 0.8163104802747322, 'eta': 0.13923385137421573}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:54:54,856]\u001b[0m Trial 75 finished with value: 1813.1004342205183 and parameters: {'min_child_weight': 33, 'alpha': 0.00026359050343577724, 'max_depth': 15, 'colsample_bytree': 0.8496990900738043, 'subsample': 0.7531966646803377, 'eta': 0.08673943882241748}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:55:01,707]\u001b[0m Trial 76 finished with value: 2233.962093813696 and parameters: {'min_child_weight': 52, 'alpha': 3.354139883119819e-05, 'max_depth': 15, 'colsample_bytree': 0.795598609484837, 'subsample': 0.8089183876372443, 'eta': 0.1015044812540318}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:55:09,414]\u001b[0m Trial 77 finished with value: 1491.2076874074692 and parameters: {'min_child_weight': 35, 'alpha': 2.666529801171744e-05, 'max_depth': 14, 'colsample_bytree': 0.7776251887973469, 'subsample': 0.9219684974570348, 'eta': 0.18377938426552232}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:55:17,837]\u001b[0m Trial 78 finished with value: 1866.1054272474923 and parameters: {'min_child_weight': 42, 'alpha': 2.0396765937480123e-05, 'max_depth': 14, 'colsample_bytree': 0.9296367502346783, 'subsample': 0.9150946394796884, 'eta': 0.19726982456932463}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:55:25,240]\u001b[0m Trial 79 finished with value: 1935.5568360325349 and parameters: {'min_child_weight': 29, 'alpha': 2.7118806562876336e-05, 'max_depth': 13, 'colsample_bytree': 0.7727121905736779, 'subsample': 0.9305530200549226, 'eta': 0.18093923978656037}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:55:32,049]\u001b[0m Trial 80 finished with value: 2332.823061379758 and parameters: {'min_child_weight': 77, 'alpha': 0.00012388870943303858, 'max_depth': 14, 'colsample_bytree': 0.94666290894329, 'subsample': 0.7271194672372868, 'eta': 0.16079425162353728}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:55:39,986]\u001b[0m Trial 81 finished with value: 1634.1946245430288 and parameters: {'min_child_weight': 21, 'alpha': 8.42504108297413e-06, 'max_depth': 15, 'colsample_bytree': 0.7288873690881761, 'subsample': 0.9707073831446361, 'eta': 0.1334708062178439}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:55:47,997]\u001b[0m Trial 82 finished with value: 1769.3233057871314 and parameters: {'min_child_weight': 22, 'alpha': 1.43814915331905e-05, 'max_depth': 15, 'colsample_bytree': 0.7330007459580935, 'subsample': 0.9861808595293107, 'eta': 0.14403200318835968}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:55:56,210]\u001b[0m Trial 83 finished with value: 2171.0114399989884 and parameters: {'min_child_weight': 54, 'alpha': 8.025234618780374e-06, 'max_depth': 14, 'colsample_bytree': 0.779499879277133, 'subsample': 0.9994891935560691, 'eta': 0.1304764865034086}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:56:04,668]\u001b[0m Trial 84 finished with value: 1692.0868256723336 and parameters: {'min_child_weight': 20, 'alpha': 5.0914308542961445e-05, 'max_depth': 15, 'colsample_bytree': 0.737430970343485, 'subsample': 0.974014655359237, 'eta': 0.16706833145973568}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:56:12,482]\u001b[0m Trial 85 finished with value: 1677.239234426462 and parameters: {'min_child_weight': 21, 'alpha': 1.0023650938995763e-05, 'max_depth': 15, 'colsample_bytree': 0.7304799373078772, 'subsample': 0.9501596261638975, 'eta': 0.11656872111367911}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:56:20,826]\u001b[0m Trial 86 finished with value: 1776.4073958063 and parameters: {'min_child_weight': 44, 'alpha': 4.568299890210328e-06, 'max_depth': 15, 'colsample_bytree': 0.6896111213336492, 'subsample': 0.9582982587384292, 'eta': 0.1150065746168903}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:56:28,464]\u001b[0m Trial 87 finished with value: 2058.0591201229513 and parameters: {'min_child_weight': 59, 'alpha': 1.1099174652270185e-05, 'max_depth': 15, 'colsample_bytree': 0.7211390717533204, 'subsample': 0.9400697764214733, 'eta': 0.12349774198790701}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:56:33,695]\u001b[0m Trial 88 finished with value: 3717.944885906379 and parameters: {'min_child_weight': 275, 'alpha': 6.822253484512628e-06, 'max_depth': 15, 'colsample_bytree': 0.8049291018735946, 'subsample': 0.9187808414147411, 'eta': 0.13480041632503112}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:56:38,881]\u001b[0m Trial 89 finished with value: 4189.542961360458 and parameters: {'min_child_weight': 94, 'alpha': 1.5858183682616507e-05, 'max_depth': 14, 'colsample_bytree': 0.7478062563020429, 'subsample': 0.9042833171183697, 'eta': 0.03102940546442572}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:56:47,485]\u001b[0m Trial 90 finished with value: 1598.6898218459883 and parameters: {'min_child_weight': 32, 'alpha': 9.207831680634736e-06, 'max_depth': 15, 'colsample_bytree': 0.823376978296522, 'subsample': 0.9499099867952183, 'eta': 0.1510727546194807}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:56:55,931]\u001b[0m Trial 91 finished with value: 1524.557802747391 and parameters: {'min_child_weight': 33, 'alpha': 2.5201018920282863e-05, 'max_depth': 15, 'colsample_bytree': 0.8195716280196942, 'subsample': 0.9568811181055914, 'eta': 0.10184688190940984}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:57:03,705]\u001b[0m Trial 92 finished with value: 1748.8159569219056 and parameters: {'min_child_weight': 46, 'alpha': 9.424510426440713e-06, 'max_depth': 15, 'colsample_bytree': 0.8297643885892971, 'subsample': 0.9643675302142444, 'eta': 0.09932168696467115}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:57:11,436]\u001b[0m Trial 93 finished with value: 1501.32284303615 and parameters: {'min_child_weight': 33, 'alpha': 3.5612498158115747e-06, 'max_depth': 15, 'colsample_bytree': 0.7751996991783625, 'subsample': 0.9358411246306205, 'eta': 0.11782582203184484}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:57:19,348]\u001b[0m Trial 94 finished with value: 1705.97299575703 and parameters: {'min_child_weight': 34, 'alpha': 3.857995894569852e-06, 'max_depth': 14, 'colsample_bytree': 0.8163232100912483, 'subsample': 0.9875262458795613, 'eta': 0.1477014863550048}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:57:26,361]\u001b[0m Trial 95 finished with value: 2123.508784220772 and parameters: {'min_child_weight': 65, 'alpha': 2.4113890123417743e-06, 'max_depth': 15, 'colsample_bytree': 0.7816459066166641, 'subsample': 0.9316400034796983, 'eta': 0.10581887582540651}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:57:31,748]\u001b[0m Trial 96 finished with value: 3563.581788517553 and parameters: {'min_child_weight': 203, 'alpha': 5.667202035370891e-06, 'max_depth': 14, 'colsample_bytree': 0.8754412017423843, 'subsample': 0.8990206259728144, 'eta': 0.08057380970721008}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:57:39,791]\u001b[0m Trial 97 finished with value: 1501.8050286734624 and parameters: {'min_child_weight': 32, 'alpha': 1.7861751941705008e-05, 'max_depth': 15, 'colsample_bytree': 0.8372684738871488, 'subsample': 0.9459533517601358, 'eta': 0.09239034252415687}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:57:47,329]\u001b[0m Trial 98 finished with value: 1847.2432580515306 and parameters: {'min_child_weight': 49, 'alpha': 3.92119608737059e-05, 'max_depth': 15, 'colsample_bytree': 0.8149851375611255, 'subsample': 0.9793770233664413, 'eta': 0.08895955128927921}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:57:54,448]\u001b[0m Trial 99 finished with value: 2080.2951341845483 and parameters: {'min_child_weight': 77, 'alpha': 2.9603265143877125e-06, 'max_depth': 14, 'colsample_bytree': 0.8394834362539701, 'subsample': 0.9482532429612065, 'eta': 0.1229919799950606}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:57:58,631]\u001b[0m Trial 100 finished with value: 4777.701666067615 and parameters: {'min_child_weight': 500, 'alpha': 1.6115019397092958e-05, 'max_depth': 15, 'colsample_bytree': 0.9297355951766776, 'subsample': 0.935484545159639, 'eta': 0.13205355378444175}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:58:06,492]\u001b[0m Trial 101 finished with value: 1509.1554976270777 and parameters: {'min_child_weight': 33, 'alpha': 2.0626042455943236e-05, 'max_depth': 15, 'colsample_bytree': 0.8084604022392408, 'subsample': 0.905516551953521, 'eta': 0.15299768126968946}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:58:14,469]\u001b[0m Trial 102 finished with value: 1611.3552837935647 and parameters: {'min_child_weight': 32, 'alpha': 2.3385805653105526e-05, 'max_depth': 15, 'colsample_bytree': 0.8385934685921069, 'subsample': 0.9064440896525329, 'eta': 0.11505379330361817}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:58:22,470]\u001b[0m Trial 103 finished with value: 1818.8093090263467 and parameters: {'min_child_weight': 42, 'alpha': 2.1393882386481743e-05, 'max_depth': 15, 'colsample_bytree': 0.8419397558882246, 'subsample': 0.8825863279799175, 'eta': 0.10343430645095346}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:58:29,705]\u001b[0m Trial 104 finished with value: 2009.3967948429497 and parameters: {'min_child_weight': 61, 'alpha': 2.956014503411732e-05, 'max_depth': 14, 'colsample_bytree': 0.8623074608541892, 'subsample': 0.9155953188944335, 'eta': 0.11140925213336154}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:58:37,376]\u001b[0m Trial 105 finished with value: 1626.552689068473 and parameters: {'min_child_weight': 30, 'alpha': 4.545764347745234e-05, 'max_depth': 15, 'colsample_bytree': 0.8298419680225716, 'subsample': 0.9007541240218095, 'eta': 0.07339865420466313}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:58:41,894]\u001b[0m Trial 106 finished with value: 1867.853119037465 and parameters: {'min_child_weight': 35, 'alpha': 0.0001034414016880447, 'max_depth': 5, 'colsample_bytree': 0.8249978004342622, 'subsample': 0.9022448869577326, 'eta': 0.0691278663240317}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:58:49,792]\u001b[0m Trial 107 finished with value: 1565.2283881374453 and parameters: {'min_child_weight': 31, 'alpha': 5.009348666360418e-05, 'max_depth': 15, 'colsample_bytree': 0.8883484340639904, 'subsample': 0.8744451823944726, 'eta': 0.07598563839606122}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:58:56,950]\u001b[0m Trial 108 finished with value: 2062.995391104955 and parameters: {'min_child_weight': 71, 'alpha': 2.2224366668429844e-05, 'max_depth': 14, 'colsample_bytree': 0.8931930533420136, 'subsample': 0.8807007422948373, 'eta': 0.1166984624233342}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:59:04,730]\u001b[0m Trial 109 finished with value: 1814.9193580029275 and parameters: {'min_child_weight': 46, 'alpha': 5.9650622688972325e-05, 'max_depth': 15, 'colsample_bytree': 0.8716673335796887, 'subsample': 0.92590783565965, 'eta': 0.09354474005460182}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:59:10,716]\u001b[0m Trial 110 finished with value: 2281.9120570364285 and parameters: {'min_child_weight': 87, 'alpha': 3.372986177179738e-05, 'max_depth': 9, 'colsample_bytree': 0.8856093994749965, 'subsample': 0.9581951859016796, 'eta': 0.09230624690495247}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:59:17,980]\u001b[0m Trial 111 finished with value: 1686.6343397448268 and parameters: {'min_child_weight': 32, 'alpha': 4.276902521533989e-05, 'max_depth': 15, 'colsample_bytree': 0.8055045676899067, 'subsample': 0.8622368013396202, 'eta': 0.07148274542045481}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:59:25,101]\u001b[0m Trial 112 finished with value: 2007.8120718179732 and parameters: {'min_child_weight': 54, 'alpha': 1.311560526491819e-05, 'max_depth': 15, 'colsample_bytree': 0.8448852102570413, 'subsample': 0.8974235520548836, 'eta': 0.07703066588022635}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:59:32,396]\u001b[0m Trial 113 finished with value: 1817.2759197884923 and parameters: {'min_child_weight': 30, 'alpha': 4.48751955909933e-05, 'max_depth': 15, 'colsample_bytree': 0.8322356921392433, 'subsample': 0.9105789062410251, 'eta': 0.0515967743895229}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:59:36,854]\u001b[0m Trial 114 finished with value: 4303.155800979507 and parameters: {'min_child_weight': 328, 'alpha': 2.2713093809692447e-05, 'max_depth': 15, 'colsample_bytree': 0.7868185528492002, 'subsample': 0.941526791833372, 'eta': 0.08304937632043097}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:59:44,749]\u001b[0m Trial 115 finished with value: 1742.417130923756 and parameters: {'min_child_weight': 43, 'alpha': 1.533530954211694e-05, 'max_depth': 15, 'colsample_bytree': 0.9061038052789713, 'subsample': 0.8921007499106481, 'eta': 0.0984306591094771}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:59:51,446]\u001b[0m Trial 116 finished with value: 2094.3294900783094 and parameters: {'min_child_weight': 60, 'alpha': 8.567784256340232e-05, 'max_depth': 14, 'colsample_bytree': 0.8642677523258169, 'subsample': 0.8735566939346048, 'eta': 0.06367926016110853}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 00:59:59,380]\u001b[0m Trial 117 finished with value: 1583.629998365378 and parameters: {'min_child_weight': 30, 'alpha': 3.559011725292504e-05, 'max_depth': 15, 'colsample_bytree': 0.8092999655241204, 'subsample': 0.9240079279534431, 'eta': 0.11334842422387263}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:00:07,722]\u001b[0m Trial 118 finished with value: 1727.008896384736 and parameters: {'min_child_weight': 42, 'alpha': 6.728529560700784e-06, 'max_depth': 14, 'colsample_bytree': 0.9994211415775155, 'subsample': 0.9249566113495553, 'eta': 0.10946050181897708}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:00:11,832]\u001b[0m Trial 119 finished with value: 4424.036505088994 and parameters: {'min_child_weight': 435, 'alpha': 0.0001967962626693566, 'max_depth': 14, 'colsample_bytree': 0.7642384589329351, 'subsample': 0.9541386419735026, 'eta': 0.1253247632520397}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:00:19,971]\u001b[0m Trial 120 finished with value: 1586.5478990000415 and parameters: {'min_child_weight': 31, 'alpha': 1.8382458593258716e-05, 'max_depth': 15, 'colsample_bytree': 0.8122809846281945, 'subsample': 0.9426109679303395, 'eta': 0.1541860595495856}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:00:28,324]\u001b[0m Trial 121 finished with value: 1558.9899361201742 and parameters: {'min_child_weight': 31, 'alpha': 1.1882673062522812e-05, 'max_depth': 15, 'colsample_bytree': 0.8493312309107198, 'subsample': 0.9367191670275687, 'eta': 0.15547854552433021}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:00:35,993]\u001b[0m Trial 122 finished with value: 1799.9046637859262 and parameters: {'min_child_weight': 51, 'alpha': 1.1049004538261941e-05, 'max_depth': 15, 'colsample_bytree': 0.8135837592434887, 'subsample': 0.9429737001481612, 'eta': 0.1538551739770547}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:00:44,330]\u001b[0m Trial 123 finished with value: 1462.4505937857086 and parameters: {'min_child_weight': 28, 'alpha': 3.053877383198809e-05, 'max_depth': 15, 'colsample_bytree': 0.8519069832515273, 'subsample': 0.9214856449097695, 'eta': 0.16515633860417725}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:00:52,045]\u001b[0m Trial 124 finished with value: 2064.750955221772 and parameters: {'min_child_weight': 69, 'alpha': 2.9135846497075213e-05, 'max_depth': 15, 'colsample_bytree': 0.8524002954763577, 'subsample': 0.9295543201171863, 'eta': 0.16927209826642223}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:01:01,036]\u001b[0m Trial 125 finished with value: 1462.2630446380392 and parameters: {'min_child_weight': 27, 'alpha': 1.6710536365433114e-05, 'max_depth': 15, 'colsample_bytree': 0.9450164444990349, 'subsample': 0.921154098335774, 'eta': 0.14073366344632576}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:01:09,724]\u001b[0m Trial 126 finished with value: 1813.6468507240415 and parameters: {'min_child_weight': 57, 'alpha': 5.8786075963934486e-05, 'max_depth': 15, 'colsample_bytree': 0.957824457979615, 'subsample': 0.9193407096641337, 'eta': 0.1851805714605178}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:01:16,871]\u001b[0m Trial 127 finished with value: 2415.61033240032 and parameters: {'min_child_weight': 42, 'alpha': 3.6296166434932194e-05, 'max_depth': 14, 'colsample_bytree': 0.9266785346552184, 'subsample': 0.9664950484854286, 'eta': 0.039151838655228755}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:01:20,804]\u001b[0m Trial 128 finished with value: 4302.320064588268 and parameters: {'min_child_weight': 234, 'alpha': 1.3256465958545326e-05, 'max_depth': 15, 'colsample_bytree': 0.9737326423335425, 'subsample': 0.5112908412380406, 'eta': 0.13907392775682498}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:01:29,262]\u001b[0m Trial 129 finished with value: 1423.6058679152745 and parameters: {'min_child_weight': 26, 'alpha': 1.794904907194458e-05, 'max_depth': 14, 'colsample_bytree': 0.9158133505151587, 'subsample': 0.9123430201853963, 'eta': 0.1640519617141896}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:01:37,854]\u001b[0m Trial 130 finished with value: 1432.2489072002375 and parameters: {'min_child_weight': 24, 'alpha': 5.166315015460062e-06, 'max_depth': 14, 'colsample_bytree': 0.9452267013525922, 'subsample': 0.8885939855307389, 'eta': 0.1619606005224246}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:01:46,308]\u001b[0m Trial 131 finished with value: 1382.8362411116948 and parameters: {'min_child_weight': 21, 'alpha': 4.852385058834498e-06, 'max_depth': 14, 'colsample_bytree': 0.9152405433811635, 'subsample': 0.8893310961984208, 'eta': 0.16431437412691952}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:01:54,876]\u001b[0m Trial 132 finished with value: 1454.2902344327624 and parameters: {'min_child_weight': 20, 'alpha': 4.686165042325577e-06, 'max_depth': 14, 'colsample_bytree': 0.9172395592839545, 'subsample': 0.8885520967148605, 'eta': 0.16673447221852333}. Best is trial 71 with value: 1378.5020486827655.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:02:03,095]\u001b[0m Trial 133 finished with value: 1229.970786942123 and parameters: {'min_child_weight': 21, 'alpha': 4.942615434527387e-06, 'max_depth': 13, 'colsample_bytree': 0.9188509285361383, 'subsample': 0.8928671489358959, 'eta': 0.16142476850744988}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:02:11,414]\u001b[0m Trial 134 finished with value: 1313.7178040700182 and parameters: {'min_child_weight': 22, 'alpha': 4.5688166448520635e-06, 'max_depth': 14, 'colsample_bytree': 0.9181666709884666, 'subsample': 0.8578072538782078, 'eta': 0.1675070552042802}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:02:19,681]\u001b[0m Trial 135 finished with value: 1412.4756402815806 and parameters: {'min_child_weight': 21, 'alpha': 4.340758983127889e-06, 'max_depth': 13, 'colsample_bytree': 0.9200237986966301, 'subsample': 0.9134039153782418, 'eta': 0.16756962975333173}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:02:27,927]\u001b[0m Trial 136 finished with value: 1364.7606557417544 and parameters: {'min_child_weight': 20, 'alpha': 4.99635355995612e-06, 'max_depth': 13, 'colsample_bytree': 0.9248521837010174, 'subsample': 0.8857476869115535, 'eta': 0.1866788294058007}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:02:35,971]\u001b[0m Trial 137 finished with value: 1433.2365835318133 and parameters: {'min_child_weight': 21, 'alpha': 4.776382386324798e-06, 'max_depth': 13, 'colsample_bytree': 0.920166826172059, 'subsample': 0.8584258744078287, 'eta': 0.18923159402064504}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:02:44,194]\u001b[0m Trial 138 finished with value: 1543.272913389702 and parameters: {'min_child_weight': 20, 'alpha': 4.948488589822375e-06, 'max_depth': 13, 'colsample_bytree': 0.9198107918188254, 'subsample': 0.8634944297108572, 'eta': 0.1739024869651272}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:02:52,333]\u001b[0m Trial 139 finished with value: 1303.506043032062 and parameters: {'min_child_weight': 20, 'alpha': 3.339094696951239e-06, 'max_depth': 12, 'colsample_bytree': 0.9444281713357117, 'subsample': 0.8873598361016102, 'eta': 0.18741170586159095}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:03:00,274]\u001b[0m Trial 140 finished with value: 1715.8995274570982 and parameters: {'min_child_weight': 21, 'alpha': 1.921575586139493e-06, 'max_depth': 12, 'colsample_bytree': 0.9405843835310365, 'subsample': 0.8895234902592261, 'eta': 0.1849105912040903}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:03:08,677]\u001b[0m Trial 141 finished with value: 1293.6377957744617 and parameters: {'min_child_weight': 20, 'alpha': 3.5012798829194927e-06, 'max_depth': 13, 'colsample_bytree': 0.9581501101945196, 'subsample': 0.8860740991035793, 'eta': 0.1891464583746091}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:03:16,831]\u001b[0m Trial 142 finished with value: 1287.2953300769855 and parameters: {'min_child_weight': 20, 'alpha': 2.674946086600911e-06, 'max_depth': 13, 'colsample_bytree': 0.9558172888887146, 'subsample': 0.842799826398782, 'eta': 0.16547126383936733}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:03:24,617]\u001b[0m Trial 143 finished with value: 1413.3828229179853 and parameters: {'min_child_weight': 22, 'alpha': 2.8768322798674486e-06, 'max_depth': 12, 'colsample_bytree': 0.9581304507453197, 'subsample': 0.8534708476768853, 'eta': 0.16487599899096742}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:03:32,569]\u001b[0m Trial 144 finished with value: 1380.073035774544 and parameters: {'min_child_weight': 20, 'alpha': 2.8097119193087597e-06, 'max_depth': 12, 'colsample_bytree': 0.9588894461423443, 'subsample': 0.8515103771315861, 'eta': 0.1989002766795255}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:03:39,915]\u001b[0m Trial 145 finished with value: 1958.1123125840447 and parameters: {'min_child_weight': 48, 'alpha': 2.780861833111265e-06, 'max_depth': 12, 'colsample_bytree': 0.9596258637727809, 'subsample': 0.8430542074986102, 'eta': 0.19986239065080696}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:03:48,300]\u001b[0m Trial 146 finished with value: 1500.915678227837 and parameters: {'min_child_weight': 20, 'alpha': 4.07775640929971e-06, 'max_depth': 13, 'colsample_bytree': 0.9786060158238322, 'subsample': 0.8533210832787315, 'eta': 0.17539120153920068}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:03:55,949]\u001b[0m Trial 147 finished with value: 1725.1860708968331 and parameters: {'min_child_weight': 20, 'alpha': 1.6039943963278517e-06, 'max_depth': 12, 'colsample_bytree': 0.9212236219666526, 'subsample': 0.8329964029868357, 'eta': 0.1632769345944398}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:04:03,470]\u001b[0m Trial 148 finished with value: 1610.6492758705924 and parameters: {'min_child_weight': 43, 'alpha': 5.5595695193537465e-06, 'max_depth': 13, 'colsample_bytree': 0.9082454511511161, 'subsample': 0.8517438011738812, 'eta': 0.19160433921334477}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:04:12,025]\u001b[0m Trial 149 finished with value: 1549.551571575627 and parameters: {'min_child_weight': 20, 'alpha': 2.3214440641312724e-06, 'max_depth': 13, 'colsample_bytree': 0.9597499269015681, 'subsample': 0.8665357481780764, 'eta': 0.18771048861705134}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:04:16,585]\u001b[0m Trial 150 finished with value: 4213.670106755377 and parameters: {'min_child_weight': 370, 'alpha': 3.2335433166454074e-06, 'max_depth': 11, 'colsample_bytree': 0.9400530653189672, 'subsample': 0.8862563181780517, 'eta': 0.16987777022379386}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:04:23,979]\u001b[0m Trial 151 finished with value: 1834.4975739854156 and parameters: {'min_child_weight': 46, 'alpha': 4.464249783953504e-06, 'max_depth': 12, 'colsample_bytree': 0.9503955253491704, 'subsample': 0.876345536720483, 'eta': 0.17734118715417277}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:04:32,474]\u001b[0m Trial 152 finished with value: 1329.3369632393626 and parameters: {'min_child_weight': 20, 'alpha': 6.331755945223092e-06, 'max_depth': 13, 'colsample_bytree': 0.9817412628189789, 'subsample': 0.8935020329621922, 'eta': 0.16251798522675298}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:04:41,005]\u001b[0m Trial 153 finished with value: 1279.0455180438444 and parameters: {'min_child_weight': 20, 'alpha': 6.807047384889625e-06, 'max_depth': 13, 'colsample_bytree': 0.9793867392698984, 'subsample': 0.8886510294591751, 'eta': 0.16410731834192166}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:04:48,830]\u001b[0m Trial 154 finished with value: 1709.7820362911405 and parameters: {'min_child_weight': 42, 'alpha': 7.3425650693903924e-06, 'max_depth': 13, 'colsample_bytree': 0.9857752354531022, 'subsample': 0.8411085174950023, 'eta': 0.18850588831133103}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:04:56,753]\u001b[0m Trial 155 finished with value: 1630.309573738265 and parameters: {'min_child_weight': 38, 'alpha': 3.5825159487905814e-06, 'max_depth': 13, 'colsample_bytree': 0.9653382812416825, 'subsample': 0.8581133330020687, 'eta': 0.19965800393815514}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:05:04,279]\u001b[0m Trial 156 finished with value: 1971.1871873977939 and parameters: {'min_child_weight': 53, 'alpha': 2.2414559417214466e-06, 'max_depth': 13, 'colsample_bytree': 0.9350735537171535, 'subsample': 0.8717860453196605, 'eta': 0.16155596382025247}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:05:12,517]\u001b[0m Trial 157 finished with value: 1564.4232568030955 and parameters: {'min_child_weight': 20, 'alpha': 6.062192773781704e-06, 'max_depth': 12, 'colsample_bytree': 0.9995273667313886, 'subsample': 0.8949166345931818, 'eta': 0.1775668147823462}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:05:20,099]\u001b[0m Trial 158 finished with value: 1640.387764963835 and parameters: {'min_child_weight': 40, 'alpha': 2.9407735622553265e-06, 'max_depth': 13, 'colsample_bytree': 0.900926112411072, 'subsample': 0.8838158062066244, 'eta': 0.1612085174963093}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:05:27,542]\u001b[0m Trial 159 finished with value: 1911.783480863728 and parameters: {'min_child_weight': 52, 'alpha': 1.459050464624096e-06, 'max_depth': 12, 'colsample_bytree': 0.977928219150829, 'subsample': 0.8783279048006792, 'eta': 0.1857056359236466}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:05:35,302]\u001b[0m Trial 160 finished with value: 1539.0621395951064 and parameters: {'min_child_weight': 39, 'alpha': 7.76706720653913e-06, 'max_depth': 13, 'colsample_bytree': 0.9538676990171611, 'subsample': 0.854848813091489, 'eta': 0.14728797919691985}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:05:43,339]\u001b[0m Trial 161 finished with value: 1452.7263459115566 and parameters: {'min_child_weight': 26, 'alpha': 4.519768149909293e-06, 'max_depth': 13, 'colsample_bytree': 0.921684406210336, 'subsample': 0.8871674810564096, 'eta': 0.167655221556601}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:05:51,172]\u001b[0m Trial 162 finished with value: 1360.9577306786216 and parameters: {'min_child_weight': 28, 'alpha': 5.301188864448457e-06, 'max_depth': 13, 'colsample_bytree': 0.9147528735570812, 'subsample': 0.8663438838087578, 'eta': 0.173624650413988}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:05:58,748]\u001b[0m Trial 163 finished with value: 1552.6910133128463 and parameters: {'min_child_weight': 20, 'alpha': 6.107506895150814e-06, 'max_depth': 11, 'colsample_bytree': 0.9669212592411843, 'subsample': 0.8676686024025353, 'eta': 0.1738760821267679}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:06:06,609]\u001b[0m Trial 164 finished with value: 1469.3363284511581 and parameters: {'min_child_weight': 28, 'alpha': 3.152691715473016e-06, 'max_depth': 13, 'colsample_bytree': 0.9319239194028248, 'subsample': 0.8469632038977931, 'eta': 0.18804918958191222}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:06:14,441]\u001b[0m Trial 165 finished with value: 1600.140275573573 and parameters: {'min_child_weight': 38, 'alpha': 1.7966815039028954e-06, 'max_depth': 13, 'colsample_bytree': 0.9141781238509062, 'subsample': 0.9096781666250393, 'eta': 0.16035493307879786}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:06:22,367]\u001b[0m Trial 166 finished with value: 1549.2562931709383 and parameters: {'min_child_weight': 28, 'alpha': 2.4504642978604784e-06, 'max_depth': 12, 'colsample_bytree': 0.985084410654381, 'subsample': 0.8946887181382507, 'eta': 0.17786780833831922}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:06:29,935]\u001b[0m Trial 167 finished with value: 1742.9712576761053 and parameters: {'min_child_weight': 48, 'alpha': 3.868759505411801e-06, 'max_depth': 13, 'colsample_bytree': 0.9489554162689924, 'subsample': 0.8255139577173801, 'eta': 0.19987631286582672}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:06:37,534]\u001b[0m Trial 168 finished with value: 1639.5955564765277 and parameters: {'min_child_weight': 39, 'alpha': 7.56936948600667e-06, 'max_depth': 12, 'colsample_bytree': 0.9683550845059263, 'subsample': 0.8746950972428834, 'eta': 0.150796959424457}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:06:45,252]\u001b[0m Trial 169 finished with value: 1552.4119317636905 and parameters: {'min_child_weight': 28, 'alpha': 5.0700196855523975e-06, 'max_depth': 13, 'colsample_bytree': 0.8975479853827992, 'subsample': 0.8639647156318493, 'eta': 0.1712130240056197}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:06:51,145]\u001b[0m Trial 170 finished with value: 3087.7345362612746 and parameters: {'min_child_weight': 173, 'alpha': 6.002450491380904e-06, 'max_depth': 13, 'colsample_bytree': 0.9322815242072049, 'subsample': 0.8363302371096503, 'eta': 0.18803384595179823}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:06:59,348]\u001b[0m Trial 171 finished with value: 1400.229271928569 and parameters: {'min_child_weight': 20, 'alpha': 4.231045940629527e-06, 'max_depth': 13, 'colsample_bytree': 0.9175964280487215, 'subsample': 0.8869033050918143, 'eta': 0.16225901464995673}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:07:07,786]\u001b[0m Trial 172 finished with value: 1440.2196512951023 and parameters: {'min_child_weight': 29, 'alpha': 3.429509356279195e-06, 'max_depth': 13, 'colsample_bytree': 0.9416890881012017, 'subsample': 0.8971824995483247, 'eta': 0.1561997011590825}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:07:15,571]\u001b[0m Trial 173 finished with value: 1524.5455572668516 and parameters: {'min_child_weight': 36, 'alpha': 9.085820630281223e-06, 'max_depth': 13, 'colsample_bytree': 0.9070945563572506, 'subsample': 0.9085949560307665, 'eta': 0.14578503762750916}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:07:23,509]\u001b[0m Trial 174 finished with value: 1444.047304276238 and parameters: {'min_child_weight': 20, 'alpha': 2.3406543116979465e-06, 'max_depth': 12, 'colsample_bytree': 0.9536518053549375, 'subsample': 0.8791409217595391, 'eta': 0.17931953432168277}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:07:31,134]\u001b[0m Trial 175 finished with value: 1617.4983951341517 and parameters: {'min_child_weight': 40, 'alpha': 4.366685294009773e-06, 'max_depth': 13, 'colsample_bytree': 0.9267234995290681, 'subsample': 0.8630104198009407, 'eta': 0.16185106405545036}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:07:38,413]\u001b[0m Trial 176 finished with value: 2182.376684160527 and parameters: {'min_child_weight': 61, 'alpha': 2.8661096938273224e-06, 'max_depth': 13, 'colsample_bytree': 0.8907728147937178, 'subsample': 0.889849360777647, 'eta': 0.17096697463901214}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:07:45,325]\u001b[0m Trial 177 finished with value: 5531.786861679674 and parameters: {'min_child_weight': 27, 'alpha': 5.197672887971268e-06, 'max_depth': 13, 'colsample_bytree': 0.988203004245183, 'subsample': 0.9010958630162278, 'eta': 0.021487285249522172}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:07:52,635]\u001b[0m Trial 178 finished with value: 1740.261528088052 and parameters: {'min_child_weight': 48, 'alpha': 6.889888651897837e-06, 'max_depth': 12, 'colsample_bytree': 0.9138760884505137, 'subsample': 0.8812664868147702, 'eta': 0.1867706430101576}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:08:00,943]\u001b[0m Trial 179 finished with value: 1314.0077878606862 and parameters: {'min_child_weight': 20, 'alpha': 9.015061991249806e-06, 'max_depth': 13, 'colsample_bytree': 0.9668065826502122, 'subsample': 0.849189893619031, 'eta': 0.1466289661427594}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:08:09,168]\u001b[0m Trial 180 finished with value: 1623.7704275176118 and parameters: {'min_child_weight': 37, 'alpha': 9.261489235375885e-06, 'max_depth': 14, 'colsample_bytree': 0.9698448143766661, 'subsample': 0.8470087225677907, 'eta': 0.14520448834987604}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:08:17,309]\u001b[0m Trial 181 finished with value: 1359.0609684733909 and parameters: {'min_child_weight': 21, 'alpha': 4.2115247291118664e-06, 'max_depth': 13, 'colsample_bytree': 0.9406195707021858, 'subsample': 0.8696105230505075, 'eta': 0.15850447413474775}. Best is trial 133 with value: 1229.970786942123.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results, study = search_boosting(mypipe, features_train, features_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1229.970786942123,\n",
       " {'min_child_weight': 21,\n",
       "  'alpha': 4.942615434527387e-06,\n",
       "  'max_depth': 13,\n",
       "  'colsample_bytree': 0.9188509285361383,\n",
       "  'subsample': 0.8928671489358959,\n",
       "  'eta': 0.16142476850744988})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.value, study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    }
   ],
   "source": [
    "params =  {\n",
    "    'min_child_weight': 21,\n",
    "    'alpha': 4.942615434527387e-06,\n",
    "    'max_depth': 13,\n",
    "    'colsample_bytree': 0.9188509285361383,\n",
    "    'subsample': 0.8928671489358959,\n",
    "    'eta': 0.16142476850744988\n",
    "  }\n",
    "pipe = Pipeline(steps=[\n",
    "            ('scaler', PowerTransformer()),\n",
    "            ('xgb', XGBRegressor(**params)),\n",
    "])\n",
    "pipe.fit(features_train,target_train)\n",
    "preds_test = pipe.predict(features_test)\n",
    "preds_train = pipe.predict(features_train)\n",
    "mse_test = mean_squared_error(y_true=np.exp(target_test), y_pred=np.exp(preds_test))\n",
    "mse_train = mean_squared_error(y_true=np.exp(target_train), y_pred=np.exp(preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 14.273936728016926\n",
      "mse test:  1229.970786942123\n",
      "rmse test:  35.07093935072346\n"
     ]
    }
   ],
   "source": [
    "print('mse train:', mse_train)\n",
    "print('mse test: ', mse_test)\n",
    "print('rmse test: ', np.sqrt(mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brutal overfit? let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = pd.read_csv('../data/processed/future.csv')\n",
    "future_features = future_data.drop(columns=['symbol', 'calendarYear', 'fillingDate', 'target'])\n",
    "future_target = future_data.target\n",
    "preds = pipe.predict(future_features)\n",
    "mse = mean_squared_error(y_true=future_target, y_pred=np.exp(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5459493656515411"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true=future_target, y_pred=np.exp(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>calendarYear</th>\n",
       "      <th>fillingDate</th>\n",
       "      <th>real</th>\n",
       "      <th>predicted</th>\n",
       "      <th>previousMarketCap</th>\n",
       "      <th>totalAssets</th>\n",
       "      <th>retainedEarningsToAssets</th>\n",
       "      <th>netDebt</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>2522.28195</td>\n",
       "      <td>580.82471</td>\n",
       "      <td>2104.10094</td>\n",
       "      <td>358.01966</td>\n",
       "      <td>0.01585</td>\n",
       "      <td>-67.11231</td>\n",
       "      <td>-1941.45724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021-07-29</td>\n",
       "      <td>2232.30744</td>\n",
       "      <td>586.79883</td>\n",
       "      <td>1689.97110</td>\n",
       "      <td>343.44115</td>\n",
       "      <td>0.17094</td>\n",
       "      <td>-70.52696</td>\n",
       "      <td>-1645.50861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-10-30</td>\n",
       "      <td>2104.10094</td>\n",
       "      <td>508.75714</td>\n",
       "      <td>1215.20182</td>\n",
       "      <td>342.59619</td>\n",
       "      <td>0.04621</td>\n",
       "      <td>-83.97965</td>\n",
       "      <td>-1595.34380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>2020</td>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>1775.95251</td>\n",
       "      <td>438.52985</td>\n",
       "      <td>1077.89170</td>\n",
       "      <td>338.67049</td>\n",
       "      <td>0.16361</td>\n",
       "      <td>-0.00738</td>\n",
       "      <td>-1337.42267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1862.05763</td>\n",
       "      <td>535.41132</td>\n",
       "      <td>1429.52317</td>\n",
       "      <td>361.12663</td>\n",
       "      <td>0.53298</td>\n",
       "      <td>-141.53104</td>\n",
       "      <td>-1326.64631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>T</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>171.42955</td>\n",
       "      <td>232.91690</td>\n",
       "      <td>217.93919</td>\n",
       "      <td>554.47522</td>\n",
       "      <td>0.07677</td>\n",
       "      <td>172.39983</td>\n",
       "      <td>61.48735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>105.69150</td>\n",
       "      <td>180.86304</td>\n",
       "      <td>167.92988</td>\n",
       "      <td>14.44950</td>\n",
       "      <td>0.94524</td>\n",
       "      <td>-5.90680</td>\n",
       "      <td>75.17154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>PENN</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>7.90313</td>\n",
       "      <td>92.74256</td>\n",
       "      <td>114.87510</td>\n",
       "      <td>16.96024</td>\n",
       "      <td>-0.00510</td>\n",
       "      <td>5.81491</td>\n",
       "      <td>84.83943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>MPWR</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>20.16374</td>\n",
       "      <td>127.15799</td>\n",
       "      <td>169.41610</td>\n",
       "      <td>1.59503</td>\n",
       "      <td>0.26775</td>\n",
       "      <td>-0.72883</td>\n",
       "      <td>106.99425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>PYPL</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>162.02229</td>\n",
       "      <td>302.62067</td>\n",
       "      <td>353.19692</td>\n",
       "      <td>76.19595</td>\n",
       "      <td>0.21813</td>\n",
       "      <td>-7.28648</td>\n",
       "      <td>140.59838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     symbol  calendarYear fillingDate       real  predicted  \\\n",
       "11     AAPL          2021  2021-10-28 2522.28195  580.82471   \n",
       "945    MSFT          2021  2021-07-29 2232.30744  586.79883   \n",
       "10     AAPL          2020  2020-10-30 2104.10094  508.75714   \n",
       "108    AMZN          2020  2021-02-03 1775.95251  438.52985   \n",
       "606   GOOGL          2021  2022-02-02 1862.05763  535.41132   \n",
       "...     ...           ...         ...        ...        ...   \n",
       "1267      T          2021  2022-02-16  171.42955  232.91690   \n",
       "1012   NVDA          2019  2019-02-21  105.69150  180.86304   \n",
       "1068   PENN          2021  2022-02-28    7.90313   92.74256   \n",
       "929    MPWR          2021  2022-02-25   20.16374  127.15799   \n",
       "1142   PYPL          2021  2022-02-03  162.02229  302.62067   \n",
       "\n",
       "      previousMarketCap  totalAssets  retainedEarningsToAssets    netDebt  \\\n",
       "11           2104.10094    358.01966                   0.01585  -67.11231   \n",
       "945          1689.97110    343.44115                   0.17094  -70.52696   \n",
       "10           1215.20182    342.59619                   0.04621  -83.97965   \n",
       "108          1077.89170    338.67049                   0.16361   -0.00738   \n",
       "606          1429.52317    361.12663                   0.53298 -141.53104   \n",
       "...                 ...          ...                       ...        ...   \n",
       "1267          217.93919    554.47522                   0.07677  172.39983   \n",
       "1012          167.92988     14.44950                   0.94524   -5.90680   \n",
       "1068          114.87510     16.96024                  -0.00510    5.81491   \n",
       "929           169.41610      1.59503                   0.26775   -0.72883   \n",
       "1142          353.19692     76.19595                   0.21813   -7.28648   \n",
       "\n",
       "           error  \n",
       "11   -1941.45724  \n",
       "945  -1645.50861  \n",
       "10   -1595.34380  \n",
       "108  -1337.42267  \n",
       "606  -1326.64631  \n",
       "...          ...  \n",
       "1267    61.48735  \n",
       "1012    75.17154  \n",
       "1068    84.83943  \n",
       "929    106.99425  \n",
       "1142   140.59838  \n",
       "\n",
       "[1494 rows x 10 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(zip(np.exp(preds), future_target), columns=['predicted', 'real'], index=future_features.index)\n",
    "df_predictions = pd.concat([future_data.loc[future_features.index,:], predictions], axis=1)\n",
    "context = ['symbol','calendarYear','fillingDate','real', 'predicted','previousMarketCap', 'totalAssets', 'retainedEarningsToAssets', 'netDebt']\n",
    "df_predictions = df_predictions.sort_values(by = ['symbol', 'calendarYear']).loc[:,context]\n",
    "df_predictions['error'] = (df_predictions.predicted - df_predictions.real)\n",
    "df_predictions.sort_values(by='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='predicted', ylabel='real'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.scatterplot(data = df_predictions, x= 'predicted', y='real', hue='calendarYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>previousMarketCap</td>\n",
       "      <td>0.52610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>freeCashFlow</td>\n",
       "      <td>0.08101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chicagoFedFinancialConditions</td>\n",
       "      <td>0.02705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wilshire5000YearOverYear</td>\n",
       "      <td>0.02586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nasdaq</td>\n",
       "      <td>0.02312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>nasdaqYearOverYear</td>\n",
       "      <td>0.01811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>inventorySalesRatioYearOverYear</td>\n",
       "      <td>0.01732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>totalAssets</td>\n",
       "      <td>0.01175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stlouisFredFinancialStress</td>\n",
       "      <td>0.01060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GDPYearOverYear</td>\n",
       "      <td>0.00901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>volatilityYearOverYear</td>\n",
       "      <td>0.00877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>revenue</td>\n",
       "      <td>0.00661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GDP</td>\n",
       "      <td>0.00651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>consumerSentimentYearOverYear</td>\n",
       "      <td>0.00615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>operatingIncomeToRevenue</td>\n",
       "      <td>0.00604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>monthlySupplyHouses</td>\n",
       "      <td>0.00551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>costOfRevenueToRevenue</td>\n",
       "      <td>0.00549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>inventorySalesRatio</td>\n",
       "      <td>0.00493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>brentYearOverYear</td>\n",
       "      <td>0.00491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>unemploymentYearOverYear</td>\n",
       "      <td>0.00487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>netIncomeToRevenue</td>\n",
       "      <td>0.00482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>interest10Y3MYearOverYear</td>\n",
       "      <td>0.00477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>moodysBaa20YYearOverYear</td>\n",
       "      <td>0.00434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>longTermInvestmentsToAssets</td>\n",
       "      <td>0.00359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>GDPSMA3</td>\n",
       "      <td>0.00356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>netIncomeToRevenueYearOverYear</td>\n",
       "      <td>0.00331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>monthlySupplyHousesSMA3</td>\n",
       "      <td>0.00319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>totalStockholdersEquityToAssets</td>\n",
       "      <td>0.00316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>revenueYoY</td>\n",
       "      <td>0.00314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>roe</td>\n",
       "      <td>0.00292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>stlouisFredFinancialStressSMA3</td>\n",
       "      <td>0.00289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>wtiYearOverYear</td>\n",
       "      <td>0.00282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ebitdaToRevenueYearOverYear</td>\n",
       "      <td>0.00264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>leadingIndexYearOverYear</td>\n",
       "      <td>0.00264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>chicagoFedFinancialConditionsSMA3</td>\n",
       "      <td>0.00261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>netSharesRepurchasedToRevenue</td>\n",
       "      <td>0.00261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>stockBasedCompensationToRevenue</td>\n",
       "      <td>0.00242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>volatility</td>\n",
       "      <td>0.00242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>unemploymentSMA3</td>\n",
       "      <td>0.00234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>stockBasedCompensationToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>netDebtToEBITDA</td>\n",
       "      <td>0.00228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>interest3MYearOverYear</td>\n",
       "      <td>0.00224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>costOfRevenueToRevenueYearOverYear</td>\n",
       "      <td>0.00220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>netReceivablesToAssets</td>\n",
       "      <td>0.00218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>mortgage30YSMA3</td>\n",
       "      <td>0.00188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>netIncomeToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>interest10YIInflationAdjustedYearOverYear</td>\n",
       "      <td>0.00187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>acquisitionsNetToRevenueYearOverYear</td>\n",
       "      <td>0.00185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>moodysBaa20Y</td>\n",
       "      <td>0.00185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>leadingIndexSMA3</td>\n",
       "      <td>0.00183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>sellingGeneralAndAdministrativeExpensesToReven...</td>\n",
       "      <td>0.00178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consumerSentiment</td>\n",
       "      <td>0.00175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ebitdaToRevenue</td>\n",
       "      <td>0.00174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>otherCurrentAssetsToAssets</td>\n",
       "      <td>0.00174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>freeCashFlowToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>interest10YYearOverYear</td>\n",
       "      <td>0.00171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>freeCashFlowToRevenueYearOverYear</td>\n",
       "      <td>0.00170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>nasdaqSMA3</td>\n",
       "      <td>0.00170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>interest3MSMA3</td>\n",
       "      <td>0.00168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>interest10YIInflationAdjusted</td>\n",
       "      <td>0.00164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>moodysAaa20YSMA3</td>\n",
       "      <td>0.00162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sellingGeneralAndAdministrativeExpensesToRevenue</td>\n",
       "      <td>0.00159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>moodysBaa20YSMA3</td>\n",
       "      <td>0.00155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>freeCashFlowYoYSMA3</td>\n",
       "      <td>0.00155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>freeCashFlowYoY</td>\n",
       "      <td>0.00154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>monthlySupplyHousesYearOverYear</td>\n",
       "      <td>0.00148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>operatingIncomeToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>freeCashFlowGivenToShareholders</td>\n",
       "      <td>0.00146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>retainedEarningsToAssets</td>\n",
       "      <td>0.00145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>interest20YYearOverYear</td>\n",
       "      <td>0.00144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>operatingIncomeToRevenueYearOverYear</td>\n",
       "      <td>0.00144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>purchasesOfInvestmentsToRevenue</td>\n",
       "      <td>0.00139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>netDebtToAssets</td>\n",
       "      <td>0.00138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>purchasesOfInvestmentsToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>otherNonCurrentAssetsToAssets</td>\n",
       "      <td>0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>stockBasedCompensationToRevenueYearOverYear</td>\n",
       "      <td>0.00136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>depreciationAndAmortizationToRevenue</td>\n",
       "      <td>0.00134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>shortTermInvestmentsToAssets</td>\n",
       "      <td>0.00133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>depreciationAndAmortizationToRevenueYearOverYear</td>\n",
       "      <td>0.00132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>changeInWorkingCapitalToRevenue</td>\n",
       "      <td>0.00132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>moodysAaa20YYearOverYear</td>\n",
       "      <td>0.00131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>netDebt</td>\n",
       "      <td>0.00128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>dividendsPaidToRevenueYearOverYear</td>\n",
       "      <td>0.00126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>changeInWorkingCapitalToRevenueYearOverYear</td>\n",
       "      <td>0.00126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>wti</td>\n",
       "      <td>0.00126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wilshire5000</td>\n",
       "      <td>0.00125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mortgage30Y</td>\n",
       "      <td>0.00123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>dividendsPaidToRevenue</td>\n",
       "      <td>0.00123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>longTermDebtToAssets</td>\n",
       "      <td>0.00122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>ebitdaToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>researchAndDevelopmentExpensesToRevenue</td>\n",
       "      <td>0.00120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>revenueYoYSMA3</td>\n",
       "      <td>0.00119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>capitalExpenditureToRevenueYearOverYear</td>\n",
       "      <td>0.00118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interest10Y</td>\n",
       "      <td>0.00117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>netDebtYoY</td>\n",
       "      <td>0.00116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mortgage30YYearOverYear</td>\n",
       "      <td>0.00116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>propertyPlantEquipmentNetToAssets</td>\n",
       "      <td>0.00115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>unemployment</td>\n",
       "      <td>0.00115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>costOfRevenueToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>otherNonCurrentLiabilitiesToAssets</td>\n",
       "      <td>0.00114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>volatilitySMA3</td>\n",
       "      <td>0.00112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>acquisitionsNetToRevenue</td>\n",
       "      <td>0.00111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>inventorySalesRatioSMA3</td>\n",
       "      <td>0.00111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>acquisitionsNetToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>sellingGeneralAndAdministrativeExpensesToReven...</td>\n",
       "      <td>0.00111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>chicagoFedFinancialConditionsYearOverYear</td>\n",
       "      <td>0.00109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>cashAndCashEquivalentsToAssets</td>\n",
       "      <td>0.00108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>capitalExpenditureToRevenue</td>\n",
       "      <td>0.00108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>corePCEYearOverYear</td>\n",
       "      <td>0.00107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>netDebtYoYSMA3</td>\n",
       "      <td>0.00107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>PPEtoSales</td>\n",
       "      <td>0.00106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>inventoryToAssets</td>\n",
       "      <td>0.00106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>capitalExpenditureToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>shortTermDebtToAssets</td>\n",
       "      <td>0.00105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>depreciationAndAmortizationToRevenueYearOverYe...</td>\n",
       "      <td>0.00104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>researchAndDevelopmentExpensesToRevenueYearOve...</td>\n",
       "      <td>0.00102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>dividendsPaidToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>moodysAaa20Y</td>\n",
       "      <td>0.00098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>interest10Y3M</td>\n",
       "      <td>0.00098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>interest10YIInflationAdjustedSMA3</td>\n",
       "      <td>0.00097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>changeInWorkingCapitalToRevenueYearOverYearSMA3</td>\n",
       "      <td>0.00095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>leadingIndex</td>\n",
       "      <td>0.00095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>deferredRevenueToAssets</td>\n",
       "      <td>0.00094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>consumerSentimentSMA3</td>\n",
       "      <td>0.00092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>purchasesOfInvestmentsToRevenueYearOverYear</td>\n",
       "      <td>0.00091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>otherCurrentLiabilitiesToAssets</td>\n",
       "      <td>0.00089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>interest10Y3MSMA3</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>accountPayablesToAssets</td>\n",
       "      <td>0.00087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>corePCESMA3</td>\n",
       "      <td>0.00087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>freeCashFlowToRevenue</td>\n",
       "      <td>0.00085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>interest20YSMA3</td>\n",
       "      <td>0.00084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brent</td>\n",
       "      <td>0.00083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>intangibleAssetsToAssets</td>\n",
       "      <td>0.00076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>interest10YSMA3</td>\n",
       "      <td>0.00070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>stlouisFredFinancialStressYearOverYear</td>\n",
       "      <td>0.00069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>brentSMA3</td>\n",
       "      <td>0.00066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corePCE</td>\n",
       "      <td>0.00064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>interest3M</td>\n",
       "      <td>0.00064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>researchAndDevelopmentExpensesToRevenueYearOve...</td>\n",
       "      <td>0.00063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>wtiSMA3</td>\n",
       "      <td>0.00059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>interest20Y</td>\n",
       "      <td>0.00053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>wilshire5000SMA3</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0       1\n",
       "66                                   previousMarketCap 0.52610\n",
       "70                                        freeCashFlow 0.08101\n",
       "1                        chicagoFedFinancialConditions 0.02705\n",
       "42                            wilshire5000YearOverYear 0.02586\n",
       "16                                              nasdaq 0.02312\n",
       "38                                  nasdaqYearOverYear 0.01811\n",
       "32                     inventorySalesRatioYearOverYear 0.01732\n",
       "67                                         totalAssets 0.01175\n",
       "17                          stlouisFredFinancialStress 0.01060\n",
       "26                                     GDPYearOverYear 0.00901\n",
       "41                              volatilityYearOverYear 0.00877\n",
       "69                                             revenue 0.00661\n",
       "4                                                  GDP 0.00651\n",
       "24                       consumerSentimentYearOverYear 0.00615\n",
       "99                            operatingIncomeToRevenue 0.00604\n",
       "12                                 monthlySupplyHouses 0.00551\n",
       "95                              costOfRevenueToRevenue 0.00549\n",
       "10                                 inventorySalesRatio 0.00493\n",
       "22                                   brentYearOverYear 0.00491\n",
       "40                            unemploymentYearOverYear 0.00487\n",
       "100                                 netIncomeToRevenue 0.00482\n",
       "28                           interest10Y3MYearOverYear 0.00477\n",
       "36                            moodysBaa20YYearOverYear 0.00434\n",
       "84                         longTermInvestmentsToAssets 0.00359\n",
       "48                                             GDPSMA3 0.00356\n",
       "106                     netIncomeToRevenueYearOverYear 0.00331\n",
       "56                             monthlySupplyHousesSMA3 0.00319\n",
       "93                     totalStockholdersEquityToAssets 0.00316\n",
       "71                                          revenueYoY 0.00314\n",
       "141                                                roe 0.00292\n",
       "61                      stlouisFredFinancialStressSMA3 0.00289\n",
       "43                                     wtiYearOverYear 0.00282\n",
       "104                        ebitdaToRevenueYearOverYear 0.00264\n",
       "33                            leadingIndexYearOverYear 0.00264\n",
       "45                   chicagoFedFinancialConditionsSMA3 0.00261\n",
       "121                      netSharesRepurchasedToRevenue 0.00261\n",
       "113                    stockBasedCompensationToRevenue 0.00242\n",
       "19                                          volatility 0.00242\n",
       "62                                    unemploymentSMA3 0.00234\n",
       "130    stockBasedCompensationToRevenueYearOverYearSMA3 0.00231\n",
       "140                                    netDebtToEBITDA 0.00228\n",
       "31                              interest3MYearOverYear 0.00224\n",
       "101                 costOfRevenueToRevenueYearOverYear 0.00220\n",
       "79                              netReceivablesToAssets 0.00218\n",
       "59                                     mortgage30YSMA3 0.00188\n",
       "112                 netIncomeToRevenueYearOverYearSMA3 0.00187\n",
       "29           interest10YIInflationAdjustedYearOverYear 0.00187\n",
       "127               acquisitionsNetToRevenueYearOverYear 0.00185\n",
       "14                                        moodysBaa20Y 0.00185\n",
       "55                                    leadingIndexSMA3 0.00183\n",
       "103  sellingGeneralAndAdministrativeExpensesToReven... 0.00178\n",
       "2                                    consumerSentiment 0.00175\n",
       "98                                     ebitdaToRevenue 0.00174\n",
       "81                          otherCurrentAssetsToAssets 0.00174\n",
       "133              freeCashFlowToRevenueYearOverYearSMA3 0.00172\n",
       "27                             interest10YYearOverYear 0.00171\n",
       "125                  freeCashFlowToRevenueYearOverYear 0.00170\n",
       "60                                          nasdaqSMA3 0.00170\n",
       "53                                      interest3MSMA3 0.00168\n",
       "7                        interest10YIInflationAdjusted 0.00164\n",
       "57                                    moodysAaa20YSMA3 0.00162\n",
       "97    sellingGeneralAndAdministrativeExpensesToRevenue 0.00159\n",
       "58                                    moodysBaa20YSMA3 0.00155\n",
       "76                                 freeCashFlowYoYSMA3 0.00155\n",
       "75                                     freeCashFlowYoY 0.00154\n",
       "34                     monthlySupplyHousesYearOverYear 0.00148\n",
       "111           operatingIncomeToRevenueYearOverYearSMA3 0.00146\n",
       "138                    freeCashFlowGivenToShareholders 0.00146\n",
       "92                            retainedEarningsToAssets 0.00145\n",
       "30                             interest20YYearOverYear 0.00144\n",
       "105               operatingIncomeToRevenueYearOverYear 0.00144\n",
       "119                    purchasesOfInvestmentsToRevenue 0.00139\n",
       "94                                     netDebtToAssets 0.00138\n",
       "136    purchasesOfInvestmentsToRevenueYearOverYearSMA3 0.00137\n",
       "85                       otherNonCurrentAssetsToAssets 0.00137\n",
       "122        stockBasedCompensationToRevenueYearOverYear 0.00136\n",
       "114               depreciationAndAmortizationToRevenue 0.00134\n",
       "78                        shortTermInvestmentsToAssets 0.00133\n",
       "123   depreciationAndAmortizationToRevenueYearOverYear 0.00132\n",
       "115                    changeInWorkingCapitalToRevenue 0.00132\n",
       "35                            moodysAaa20YYearOverYear 0.00131\n",
       "68                                             netDebt 0.00128\n",
       "129                 dividendsPaidToRevenueYearOverYear 0.00126\n",
       "124        changeInWorkingCapitalToRevenueYearOverYear 0.00126\n",
       "21                                                 wti 0.00126\n",
       "20                                        wilshire5000 0.00125\n",
       "15                                         mortgage30Y 0.00123\n",
       "120                             dividendsPaidToRevenue 0.00123\n",
       "90                                longTermDebtToAssets 0.00122\n",
       "110                    ebitdaToRevenueYearOverYearSMA3 0.00121\n",
       "96             researchAndDevelopmentExpensesToRevenue 0.00120\n",
       "72                                      revenueYoYSMA3 0.00119\n",
       "126            capitalExpenditureToRevenueYearOverYear 0.00118\n",
       "5                                          interest10Y 0.00117\n",
       "73                                          netDebtYoY 0.00116\n",
       "37                             mortgage30YYearOverYear 0.00116\n",
       "82                   propertyPlantEquipmentNetToAssets 0.00115\n",
       "18                                        unemployment 0.00115\n",
       "107             costOfRevenueToRevenueYearOverYearSMA3 0.00114\n",
       "91                  otherNonCurrentLiabilitiesToAssets 0.00114\n",
       "63                                      volatilitySMA3 0.00112\n",
       "118                           acquisitionsNetToRevenue 0.00111\n",
       "54                             inventorySalesRatioSMA3 0.00111\n",
       "135           acquisitionsNetToRevenueYearOverYearSMA3 0.00111\n",
       "109  sellingGeneralAndAdministrativeExpensesToReven... 0.00111\n",
       "23           chicagoFedFinancialConditionsYearOverYear 0.00109\n",
       "77                      cashAndCashEquivalentsToAssets 0.00108\n",
       "117                        capitalExpenditureToRevenue 0.00108\n",
       "25                                 corePCEYearOverYear 0.00107\n",
       "74                                      netDebtYoYSMA3 0.00107\n",
       "139                                         PPEtoSales 0.00106\n",
       "80                                   inventoryToAssets 0.00106\n",
       "134        capitalExpenditureToRevenueYearOverYearSMA3 0.00105\n",
       "87                               shortTermDebtToAssets 0.00105\n",
       "131  depreciationAndAmortizationToRevenueYearOverYe... 0.00104\n",
       "108  researchAndDevelopmentExpensesToRevenueYearOve... 0.00102\n",
       "137             dividendsPaidToRevenueYearOverYearSMA3 0.00100\n",
       "13                                        moodysAaa20Y 0.00098\n",
       "6                                        interest10Y3M 0.00098\n",
       "51                   interest10YIInflationAdjustedSMA3 0.00097\n",
       "132    changeInWorkingCapitalToRevenueYearOverYearSMA3 0.00095\n",
       "11                                        leadingIndex 0.00095\n",
       "88                             deferredRevenueToAssets 0.00094\n",
       "46                               consumerSentimentSMA3 0.00092\n",
       "128        purchasesOfInvestmentsToRevenueYearOverYear 0.00091\n",
       "89                     otherCurrentLiabilitiesToAssets 0.00089\n",
       "50                                   interest10Y3MSMA3 0.00088\n",
       "86                             accountPayablesToAssets 0.00087\n",
       "47                                         corePCESMA3 0.00087\n",
       "116                              freeCashFlowToRevenue 0.00085\n",
       "52                                     interest20YSMA3 0.00084\n",
       "0                                                brent 0.00083\n",
       "83                            intangibleAssetsToAssets 0.00076\n",
       "49                                     interest10YSMA3 0.00070\n",
       "39              stlouisFredFinancialStressYearOverYear 0.00069\n",
       "44                                           brentSMA3 0.00066\n",
       "3                                              corePCE 0.00064\n",
       "9                                           interest3M 0.00064\n",
       "102  researchAndDevelopmentExpensesToRevenueYearOve... 0.00063\n",
       "65                                             wtiSMA3 0.00059\n",
       "8                                          interest20Y 0.00053\n",
       "64                                    wilshire5000SMA3 0.00000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(future_features.columns, pipe['xgb'].feature_importances_)).sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to predict increases instead of absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_linear_regression(pipe, X_train, X_test, y_train, y_test=None):\n",
    "    pipe.steps.append(('linear_regression', LinearRegression()))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds_test = pipe.predict(X_test)\n",
    "    preds_train = pipe.predict(X_train)\n",
    "    mse_test = mean_squared_error(y_true=y_test, y_pred=preds_test)\n",
    "    mse_train = mean_squared_error(y_true=y_train, y_pred=preds_train)\n",
    "    print('mse train:', mse_train)\n",
    "    print('mse test: ', mse_test)\n",
    "    print('rmse test: ', np.sqrt(mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_boosting(pipe, X_train, X_test, y_train, y_test=None):\n",
    "    def objective(trial):\n",
    "        params={\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"booster\": \"gbtree\",\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 20, 500),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-6, 1.0, log=True),\n",
    "            \"max_depth\" : trial.suggest_int(\"max_depth\", 5, 10),\n",
    "            \"colsample_bytree\" : trial.suggest_float(\"colsample_bytree\", 0.4, 1),\n",
    "            \"subsample\" : trial.suggest_float(\"subsample\", 0.5, 1),\n",
    "            \"eta\" : trial.suggest_float(\"eta\", 1e-2, 0.2, log=True)\n",
    "            }\n",
    "        pipe = Pipeline(steps=[\n",
    "            ('scaler', PowerTransformer()),\n",
    "            ('xgb', XGBRegressor(**params)),\n",
    "            ])\n",
    "        pipe.fit(X_train,y_train)\n",
    "        preds = pipe.predict(X_test)\n",
    "        mse = mean_squared_error(y_true=y_test, y_pred=preds)\n",
    "        return mse\n",
    "\n",
    "    minutes = 10\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, timeout=(60*minutes))\n",
    "    results = study.trials_dataframe()\n",
    "    return results, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1435, 147), (4883, 147))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/processed/present.csv')\n",
    "data.query(\"calendarYear >= 2016\").shape, data.query(\"calendarYear < 2016\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = data.query(\"calendarYear > 2016\").drop(columns=['target', 'symbol', 'calendarYear', 'fillingDate', 'targetYoY'])\n",
    "target_test = data.query(\"calendarYear > 2016\").targetYoY\n",
    "features_train = data.query(\"calendarYear <= 2016\").drop(columns=['target', 'symbol', 'calendarYear', 'fillingDate', 'targetYoY'])\n",
    "target_train = data.query(\"calendarYear <= 2016\").targetYoY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5351, 142), (967, 142), (5351,), (967,)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in [features_train, features_test, target_train, target_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 0.18424784182139747\n",
      "mse test:  0.13064282476312716\n",
      "rmse test:  0.361445465821785\n"
     ]
    }
   ],
   "source": [
    "mypipe = Pipeline(steps=[\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ])\n",
    "do_linear_regression(mypipe, features_train, features_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-20 01:49:15,044]\u001b[0m A new study created in memory with name: no-name-ea341820-56ba-4b22-9688-28eff2b1e31c\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:19,334]\u001b[0m Trial 0 finished with value: 0.11757116280691793 and parameters: {'min_child_weight': 417, 'alpha': 0.35119620843006905, 'max_depth': 10, 'colsample_bytree': 0.8009761819753494, 'subsample': 0.9043000123164997, 'eta': 0.13194187677159233}. Best is trial 0 with value: 0.11757116280691793.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:23,591]\u001b[0m Trial 1 finished with value: 0.11274633140295541 and parameters: {'min_child_weight': 60, 'alpha': 0.05374809392781862, 'max_depth': 6, 'colsample_bytree': 0.8961497843168762, 'subsample': 0.5710812380635979, 'eta': 0.034184706069148814}. Best is trial 1 with value: 0.11274633140295541.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:28,606]\u001b[0m Trial 2 finished with value: 0.11264614123041694 and parameters: {'min_child_weight': 81, 'alpha': 0.004827592060566049, 'max_depth': 10, 'colsample_bytree': 0.6112539570098101, 'subsample': 0.8436367376136125, 'eta': 0.05693665157184406}. Best is trial 2 with value: 0.11264614123041694.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:31,907]\u001b[0m Trial 3 finished with value: 0.12121174424179279 and parameters: {'min_child_weight': 276, 'alpha': 0.00018302490414012418, 'max_depth': 9, 'colsample_bytree': 0.4663319818092334, 'subsample': 0.5408554338592123, 'eta': 0.15451484051608527}. Best is trial 2 with value: 0.11264614123041694.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:36,433]\u001b[0m Trial 4 finished with value: 0.1126629798566506 and parameters: {'min_child_weight': 156, 'alpha': 0.002977306571777365, 'max_depth': 8, 'colsample_bytree': 0.8749817357561351, 'subsample': 0.6078285096845811, 'eta': 0.11479371540872446}. Best is trial 2 with value: 0.11264614123041694.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:39,718]\u001b[0m Trial 5 finished with value: 0.11866299440210148 and parameters: {'min_child_weight': 434, 'alpha': 5.375069660612174e-06, 'max_depth': 10, 'colsample_bytree': 0.8248448998506448, 'subsample': 0.5492122218404366, 'eta': 0.03922590840314619}. Best is trial 2 with value: 0.11264614123041694.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:44,392]\u001b[0m Trial 6 finished with value: 0.10985411538198003 and parameters: {'min_child_weight': 49, 'alpha': 2.1493132036965178e-06, 'max_depth': 6, 'colsample_bytree': 0.9965157096935423, 'subsample': 0.6703289722671263, 'eta': 0.05483410309934784}. Best is trial 6 with value: 0.10985411538198003.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:48,285]\u001b[0m Trial 7 finished with value: 0.12445996127564599 and parameters: {'min_child_weight': 450, 'alpha': 0.0002482967019472272, 'max_depth': 9, 'colsample_bytree': 0.8281987459387385, 'subsample': 0.7787214908975109, 'eta': 0.022561517647350086}. Best is trial 6 with value: 0.10985411538198003.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:52,261]\u001b[0m Trial 8 finished with value: 0.11759775017533723 and parameters: {'min_child_weight': 124, 'alpha': 0.00022164848616638874, 'max_depth': 6, 'colsample_bytree': 0.8379659572576477, 'subsample': 0.5441132179466261, 'eta': 0.02987187269481355}. Best is trial 6 with value: 0.10985411538198003.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:56,405]\u001b[0m Trial 9 finished with value: 0.1153457302015845 and parameters: {'min_child_weight': 121, 'alpha': 0.0025187353512690033, 'max_depth': 6, 'colsample_bytree': 0.667944562469764, 'subsample': 0.9408524088873433, 'eta': 0.038030424499815106}. Best is trial 6 with value: 0.10985411538198003.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:49:59,582]\u001b[0m Trial 10 finished with value: 0.14577633963706885 and parameters: {'min_child_weight': 255, 'alpha': 1.6538333727290577e-06, 'max_depth': 5, 'colsample_bytree': 0.40304363466886994, 'subsample': 0.6856049656772857, 'eta': 0.013412388306341708}. Best is trial 6 with value: 0.10985411538198003.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:04,134]\u001b[0m Trial 11 finished with value: 0.11207949080931047 and parameters: {'min_child_weight': 22, 'alpha': 2.441705672516671e-05, 'max_depth': 7, 'colsample_bytree': 0.6048546492607232, 'subsample': 0.8041577819804115, 'eta': 0.06962000622947494}. Best is trial 6 with value: 0.10985411538198003.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:09,439]\u001b[0m Trial 12 finished with value: 0.10808294923643179 and parameters: {'min_child_weight': 20, 'alpha': 2.0414604809452593e-05, 'max_depth': 7, 'colsample_bytree': 0.970985550599485, 'subsample': 0.7123751059907419, 'eta': 0.08096965891627209}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:15,147]\u001b[0m Trial 13 finished with value: 0.11268396122049527 and parameters: {'min_child_weight': 206, 'alpha': 1.4905452044852375e-05, 'max_depth': 7, 'colsample_bytree': 0.9564717396773859, 'subsample': 0.7060050405226111, 'eta': 0.08398799351138664}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:19,756]\u001b[0m Trial 14 finished with value: 0.11098517936933135 and parameters: {'min_child_weight': 22, 'alpha': 1.14037649840717e-06, 'max_depth': 5, 'colsample_bytree': 0.9891313619033022, 'subsample': 0.6527894409828268, 'eta': 0.09233837928533764}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:24,036]\u001b[0m Trial 15 finished with value: 0.11685981833139772 and parameters: {'min_child_weight': 318, 'alpha': 3.49303851220842e-05, 'max_depth': 8, 'colsample_bytree': 0.7354939341964432, 'subsample': 0.7420609947582897, 'eta': 0.19778656197121833}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:28,620]\u001b[0m Trial 16 finished with value: 0.12484007783676714 and parameters: {'min_child_weight': 166, 'alpha': 5.734093794416043e-06, 'max_depth': 7, 'colsample_bytree': 0.9562385047534551, 'subsample': 0.6353065576725653, 'eta': 0.020520897223130816}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:33,433]\u001b[0m Trial 17 finished with value: 0.11340393142919343 and parameters: {'min_child_weight': 79, 'alpha': 5.424631360618219e-05, 'max_depth': 6, 'colsample_bytree': 0.9244659227772323, 'subsample': 0.7314541010498653, 'eta': 0.05479290800202293}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:37,534]\u001b[0m Trial 18 finished with value: 0.11644372152100015 and parameters: {'min_child_weight': 328, 'alpha': 6.084357945242376e-06, 'max_depth': 5, 'colsample_bytree': 0.987190602337872, 'subsample': 0.8217863533328134, 'eta': 0.054319788852979216}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:41,331]\u001b[0m Trial 19 finished with value: 0.11782844319564108 and parameters: {'min_child_weight': 499, 'alpha': 1.139884045497494e-06, 'max_depth': 8, 'colsample_bytree': 0.7379174811315237, 'subsample': 0.8666489521981255, 'eta': 0.08489962921375008}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:45,461]\u001b[0m Trial 20 finished with value: 0.15659476406479167 and parameters: {'min_child_weight': 215, 'alpha': 0.00010220265366544855, 'max_depth': 7, 'colsample_bytree': 0.536389587585216, 'subsample': 0.6787242980092246, 'eta': 0.010641276825958714}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:49,879]\u001b[0m Trial 21 finished with value: 0.11163271327377575 and parameters: {'min_child_weight': 30, 'alpha': 1.4933590042674707e-06, 'max_depth': 5, 'colsample_bytree': 0.9871088053678672, 'subsample': 0.6447105971529072, 'eta': 0.09593431587593194}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:54,026]\u001b[0m Trial 22 finished with value: 0.10895044099008958 and parameters: {'min_child_weight': 21, 'alpha': 5.6620054577685e-06, 'max_depth': 5, 'colsample_bytree': 0.9130933430367545, 'subsample': 0.6140470122129104, 'eta': 0.06933845830011316}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:50:58,040]\u001b[0m Trial 23 finished with value: 0.11133131663325219 and parameters: {'min_child_weight': 102, 'alpha': 9.8988535512543e-06, 'max_depth': 6, 'colsample_bytree': 0.8794785406681671, 'subsample': 0.5019205840246018, 'eta': 0.07007146687888796}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:02,543]\u001b[0m Trial 24 finished with value: 0.11294456984932909 and parameters: {'min_child_weight': 62, 'alpha': 0.0005518335194138978, 'max_depth': 6, 'colsample_bytree': 0.9333033382004675, 'subsample': 0.5999661881905498, 'eta': 0.048301529994115286}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:06,458]\u001b[0m Trial 25 finished with value: 0.11372031664097915 and parameters: {'min_child_weight': 144, 'alpha': 2.810975830547931e-06, 'max_depth': 5, 'colsample_bytree': 0.7688239635249456, 'subsample': 0.7720641328370876, 'eta': 0.06497498806457215}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:11,935]\u001b[0m Trial 26 finished with value: 0.11274314676561763 and parameters: {'min_child_weight': 58, 'alpha': 2.3442327298241468e-05, 'max_depth': 7, 'colsample_bytree': 0.9019984036729592, 'subsample': 0.9865083101832606, 'eta': 0.1109245412093978}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:16,258]\u001b[0m Trial 27 finished with value: 0.11412903842639945 and parameters: {'min_child_weight': 103, 'alpha': 5.629780274333593e-05, 'max_depth': 6, 'colsample_bytree': 0.8601876144062675, 'subsample': 0.7125957857638925, 'eta': 0.02869011422735182}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:20,207]\u001b[0m Trial 28 finished with value: 0.11542108527681709 and parameters: {'min_child_weight': 192, 'alpha': 0.013015261213134583, 'max_depth': 5, 'colsample_bytree': 0.9377681802313426, 'subsample': 0.6212917606683885, 'eta': 0.046967080343106746}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:24,004]\u001b[0m Trial 29 finished with value: 0.11873537574899266 and parameters: {'min_child_weight': 355, 'alpha': 0.20429127569366035, 'max_depth': 7, 'colsample_bytree': 0.7917015035620507, 'subsample': 0.6827403408112036, 'eta': 0.06780187236217793}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:28,683]\u001b[0m Trial 30 finished with value: 0.11037920624590457 and parameters: {'min_child_weight': 32, 'alpha': 2.9036128835877056e-06, 'max_depth': 6, 'colsample_bytree': 0.9850756628654158, 'subsample': 0.5920125725753511, 'eta': 0.14073986824932472}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:33,223]\u001b[0m Trial 31 finished with value: 0.10911491106845769 and parameters: {'min_child_weight': 45, 'alpha': 3.4483279186332117e-06, 'max_depth': 6, 'colsample_bytree': 0.9843342907699586, 'subsample': 0.5810892817467701, 'eta': 0.13961460389750682}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:37,757]\u001b[0m Trial 32 finished with value: 0.1115561554323939 and parameters: {'min_child_weight': 65, 'alpha': 8.796763823552988e-06, 'max_depth': 6, 'colsample_bytree': 0.912095477518165, 'subsample': 0.6686301401590248, 'eta': 0.16264533116445898}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:41,879]\u001b[0m Trial 33 finished with value: 0.11273201916772493 and parameters: {'min_child_weight': 45, 'alpha': 4.039096046529168e-06, 'max_depth': 5, 'colsample_bytree': 0.9543788512263919, 'subsample': 0.57416251413556, 'eta': 0.11228228102289507}. Best is trial 12 with value: 0.10808294923643179.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:46,380]\u001b[0m Trial 34 finished with value: 0.10695923659803186 and parameters: {'min_child_weight': 92, 'alpha': 1.302340784776789e-05, 'max_depth': 6, 'colsample_bytree': 0.9986995561581491, 'subsample': 0.5744858994590477, 'eta': 0.192269391916285}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:50,665]\u001b[0m Trial 35 finished with value: 0.11440819093394744 and parameters: {'min_child_weight': 92, 'alpha': 0.0006552707494484242, 'max_depth': 7, 'colsample_bytree': 0.8935975863754835, 'subsample': 0.5006555105330547, 'eta': 0.1983132246627707}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:55,401]\u001b[0m Trial 36 finished with value: 0.11323673639541258 and parameters: {'min_child_weight': 133, 'alpha': 1.511081660848309e-05, 'max_depth': 8, 'colsample_bytree': 0.8553509578741039, 'subsample': 0.5834346238651992, 'eta': 0.16369837243103635}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:51:59,931]\u001b[0m Trial 37 finished with value: 0.11219872495920002 and parameters: {'min_child_weight': 73, 'alpha': 0.7706490724943635, 'max_depth': 6, 'colsample_bytree': 0.9535296342689975, 'subsample': 0.5399150513639946, 'eta': 0.12969028188097312}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:04,204]\u001b[0m Trial 38 finished with value: 0.11296170664234301 and parameters: {'min_child_weight': 172, 'alpha': 8.687099918136242e-05, 'max_depth': 5, 'colsample_bytree': 0.9998758277063394, 'subsample': 0.616084344667977, 'eta': 0.132622088434515}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:08,868]\u001b[0m Trial 39 finished with value: 0.11451768306353957 and parameters: {'min_child_weight': 108, 'alpha': 0.024444460844588855, 'max_depth': 7, 'colsample_bytree': 0.8996267901760836, 'subsample': 0.5615075044850653, 'eta': 0.16501768803696287}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:13,892]\u001b[0m Trial 40 finished with value: 0.11335970780134691 and parameters: {'min_child_weight': 50, 'alpha': 1.3149874270978216e-05, 'max_depth': 9, 'colsample_bytree': 0.8044929366080946, 'subsample': 0.5318004963956646, 'eta': 0.10099773473529382}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:18,524]\u001b[0m Trial 41 finished with value: 0.1115495563093029 and parameters: {'min_child_weight': 45, 'alpha': 2.4294985081963527e-06, 'max_depth': 6, 'colsample_bytree': 0.9646345531033343, 'subsample': 0.6502464160964332, 'eta': 0.0816787282664859}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:22,882]\u001b[0m Trial 42 finished with value: 0.11221575584550242 and parameters: {'min_child_weight': 82, 'alpha': 4.302933168275244e-06, 'max_depth': 6, 'colsample_bytree': 0.9250850725513796, 'subsample': 0.6192481374165941, 'eta': 0.04067904673508682}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:26,965]\u001b[0m Trial 43 finished with value: 0.1123231432936257 and parameters: {'min_child_weight': 26, 'alpha': 2.7487979474979594e-05, 'max_depth': 6, 'colsample_bytree': 0.6493745459688218, 'subsample': 0.7043355455210856, 'eta': 0.05920408111196801}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:31,381]\u001b[0m Trial 44 finished with value: 0.10974037522390602 and parameters: {'min_child_weight': 48, 'alpha': 7.390325274613824e-06, 'max_depth': 5, 'colsample_bytree': 0.9686727733164318, 'subsample': 0.7621312649216114, 'eta': 0.07584442448584912}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:35,735]\u001b[0m Trial 45 finished with value: 0.10908198931096912 and parameters: {'min_child_weight': 119, 'alpha': 0.00011753254339260047, 'max_depth': 5, 'colsample_bytree': 0.9736422429156638, 'subsample': 0.7602840408905768, 'eta': 0.12966223243735214}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:40,192]\u001b[0m Trial 46 finished with value: 0.1101645835879672 and parameters: {'min_child_weight': 122, 'alpha': 0.00016301832282281795, 'max_depth': 5, 'colsample_bytree': 0.936959604915241, 'subsample': 0.8835025648642444, 'eta': 0.1239305106557437}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:44,112]\u001b[0m Trial 47 finished with value: 0.11083961692328968 and parameters: {'min_child_weight': 245, 'alpha': 0.0003813583416163522, 'max_depth': 5, 'colsample_bytree': 0.8354352420929183, 'subsample': 0.7906213083125855, 'eta': 0.14772810722488564}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:49,702]\u001b[0m Trial 48 finished with value: 0.11204898812353158 and parameters: {'min_child_weight': 149, 'alpha': 0.0016654142135590726, 'max_depth': 10, 'colsample_bytree': 0.8748238184368787, 'subsample': 0.8220539506987063, 'eta': 0.10712131534530629}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:53,126]\u001b[0m Trial 49 finished with value: 0.11378999040848865 and parameters: {'min_child_weight': 91, 'alpha': 4.2090937769866506e-05, 'max_depth': 5, 'colsample_bytree': 0.5530826918435431, 'subsample': 0.5608322198058054, 'eta': 0.17771514938233648}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:52:58,316]\u001b[0m Trial 50 finished with value: 0.1093124681047176 and parameters: {'min_child_weight': 21, 'alpha': 0.00010697135451548473, 'max_depth': 8, 'colsample_bytree': 0.972903573483242, 'subsample': 0.5223318720492298, 'eta': 0.12081070419902187}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:03,589]\u001b[0m Trial 51 finished with value: 0.10977737229753408 and parameters: {'min_child_weight': 73, 'alpha': 0.00011418838240491203, 'max_depth': 8, 'colsample_bytree': 0.9686241571696583, 'subsample': 0.5202052140633096, 'eta': 0.119039093093884}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:09,725]\u001b[0m Trial 52 finished with value: 0.11543969030560969 and parameters: {'min_child_weight': 23, 'alpha': 1.795121953005409e-05, 'max_depth': 9, 'colsample_bytree': 0.939844068456512, 'subsample': 0.5563854166648099, 'eta': 0.14391212298259135}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:15,208]\u001b[0m Trial 53 finished with value: 0.1089720581605095 and parameters: {'min_child_weight': 20, 'alpha': 0.00023925832744853924, 'max_depth': 8, 'colsample_bytree': 0.9969938883402498, 'subsample': 0.5225974992978848, 'eta': 0.09019856728839906}. Best is trial 34 with value: 0.10695923659803186.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:21,420]\u001b[0m Trial 54 finished with value: 0.10629498010912801 and parameters: {'min_child_weight': 46, 'alpha': 5.9160173595892574e-05, 'max_depth': 8, 'colsample_bytree': 0.9945839220286206, 'subsample': 0.7340892728126234, 'eta': 0.09104498149375055}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:26,887]\u001b[0m Trial 55 finished with value: 0.10816921881467213 and parameters: {'min_child_weight': 114, 'alpha': 5.7137037849771574e-05, 'max_depth': 8, 'colsample_bytree': 0.9144819251240944, 'subsample': 0.746842537827679, 'eta': 0.09138012695941923}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:32,439]\u001b[0m Trial 56 finished with value: 0.10763824158207629 and parameters: {'min_child_weight': 65, 'alpha': 0.00035582111514408793, 'max_depth': 8, 'colsample_bytree': 0.9976790567447833, 'subsample': 0.7263019182499655, 'eta': 0.08908545910765862}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:37,954]\u001b[0m Trial 57 finished with value: 0.10860356215199815 and parameters: {'min_child_weight': 82, 'alpha': 5.7102184964270506e-05, 'max_depth': 9, 'colsample_bytree': 0.920300146946649, 'subsample': 0.7330996496133648, 'eta': 0.08036685704012775}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:41,686]\u001b[0m Trial 58 finished with value: 0.11499362644209098 and parameters: {'min_child_weight': 171, 'alpha': 6.232390236723495e-05, 'max_depth': 9, 'colsample_bytree': 0.4125492638429907, 'subsample': 0.7275264400364464, 'eta': 0.0795427074290275}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:47,203]\u001b[0m Trial 59 finished with value: 0.1101437735183173 and parameters: {'min_child_weight': 89, 'alpha': 0.0013556142867295774, 'max_depth': 9, 'colsample_bytree': 0.8766029136893101, 'subsample': 0.7442701989161585, 'eta': 0.09697078737995186}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:52,439]\u001b[0m Trial 60 finished with value: 0.11364094051351552 and parameters: {'min_child_weight': 107, 'alpha': 0.0003381408993612736, 'max_depth': 8, 'colsample_bytree': 0.948517675650662, 'subsample': 0.7949659566696877, 'eta': 0.03232763627757196}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:53:57,676]\u001b[0m Trial 61 finished with value: 0.10856181252285863 and parameters: {'min_child_weight': 60, 'alpha': 3.331570821509066e-05, 'max_depth': 8, 'colsample_bytree': 0.9094215946128508, 'subsample': 0.7009716534769816, 'eta': 0.060650390759448794}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:02,920]\u001b[0m Trial 62 finished with value: 0.11017780198297876 and parameters: {'min_child_weight': 66, 'alpha': 4.185847166285586e-05, 'max_depth': 8, 'colsample_bytree': 0.9188407982856368, 'subsample': 0.7197139119684987, 'eta': 0.0636165688420142}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:08,235]\u001b[0m Trial 63 finished with value: 0.11279944936687478 and parameters: {'min_child_weight': 137, 'alpha': 2.5466959614291922e-05, 'max_depth': 9, 'colsample_bytree': 0.9989212514432209, 'subsample': 0.7073991933767529, 'eta': 0.05144100029580491}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:12,241]\u001b[0m Trial 64 finished with value: 0.11711888038618791 and parameters: {'min_child_weight': 394, 'alpha': 6.139136695435093e-05, 'max_depth': 8, 'colsample_bytree': 0.8870369843253878, 'subsample': 0.7440984475317652, 'eta': 0.07705144389738479}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:16,618]\u001b[0m Trial 65 finished with value: 0.11884929899981451 and parameters: {'min_child_weight': 282, 'alpha': 0.0001801711173255624, 'max_depth': 8, 'colsample_bytree': 0.9268406490825651, 'subsample': 0.6856943437346453, 'eta': 0.05869373248771599}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:21,641]\u001b[0m Trial 66 finished with value: 0.11113215033071301 and parameters: {'min_child_weight': 83, 'alpha': 3.3501647475768845e-05, 'max_depth': 9, 'colsample_bytree': 0.714846237623473, 'subsample': 0.6957845031794314, 'eta': 0.0889359794337797}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:27,186]\u001b[0m Trial 67 finished with value: 0.10962853194553235 and parameters: {'min_child_weight': 60, 'alpha': 1.3047354626626223e-05, 'max_depth': 8, 'colsample_bytree': 0.9495256626854448, 'subsample': 0.6649347988414551, 'eta': 0.0728587082942034}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:32,657]\u001b[0m Trial 68 finished with value: 0.11271627180989525 and parameters: {'min_child_weight': 41, 'alpha': 0.0006977402305135583, 'max_depth': 7, 'colsample_bytree': 0.8613165447389871, 'subsample': 0.7788508132565138, 'eta': 0.04186538935615721}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:38,281]\u001b[0m Trial 69 finished with value: 0.10990711248776515 and parameters: {'min_child_weight': 99, 'alpha': 0.0034002205686162424, 'max_depth': 8, 'colsample_bytree': 0.9066620707444364, 'subsample': 0.8153187439931696, 'eta': 0.06175160444792489}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:43,678]\u001b[0m Trial 70 finished with value: 0.11098691820342514 and parameters: {'min_child_weight': 117, 'alpha': 7.873063516984357e-05, 'max_depth': 10, 'colsample_bytree': 0.8159032892218375, 'subsample': 0.7313877047470528, 'eta': 0.05084823156449495}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:49,579]\u001b[0m Trial 71 finished with value: 0.11237646356365398 and parameters: {'min_child_weight': 37, 'alpha': 9.367995094269844e-06, 'max_depth': 7, 'colsample_bytree': 0.9126039179444592, 'subsample': 0.7617492707588857, 'eta': 0.06949036064880476}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:54:55,155]\u001b[0m Trial 72 finished with value: 0.11234125560725138 and parameters: {'min_child_weight': 56, 'alpha': 2.2817473513828247e-05, 'max_depth': 8, 'colsample_bytree': 0.9723131431632321, 'subsample': 0.6300163855967349, 'eta': 0.08566497981200882}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:00,591]\u001b[0m Trial 73 finished with value: 0.11069470470044128 and parameters: {'min_child_weight': 72, 'alpha': 4.189800627801747e-05, 'max_depth': 7, 'colsample_bytree': 0.9820029150228657, 'subsample': 0.6898557470041913, 'eta': 0.10172039637377762}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:06,379]\u001b[0m Trial 74 finished with value: 0.1125802792744667 and parameters: {'min_child_weight': 35, 'alpha': 1.9121020434366065e-05, 'max_depth': 8, 'colsample_bytree': 0.9436180372225641, 'subsample': 0.7285503524682658, 'eta': 0.0935847686242245}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:12,729]\u001b[0m Trial 75 finished with value: 0.12441191624514814 and parameters: {'min_child_weight': 57, 'alpha': 1.1322480170696062e-05, 'max_depth': 9, 'colsample_bytree': 0.9584175738052783, 'subsample': 0.7549489531170273, 'eta': 0.0168478953868914}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:17,980]\u001b[0m Trial 76 finished with value: 0.11272486603258547 and parameters: {'min_child_weight': 80, 'alpha': 4.6946835251233614e-06, 'max_depth': 8, 'colsample_bytree': 0.8542241642222452, 'subsample': 0.775128492959265, 'eta': 0.036534696462676534}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:22,624]\u001b[0m Trial 77 finished with value: 0.11197817820505672 and parameters: {'min_child_weight': 96, 'alpha': 0.00015869360760979194, 'max_depth': 7, 'colsample_bytree': 0.8953854060118039, 'subsample': 0.6001374810172976, 'eta': 0.06525865460272769}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:28,593]\u001b[0m Trial 78 finished with value: 0.10907897033826557 and parameters: {'min_child_weight': 37, 'alpha': 0.00033171382762700656, 'max_depth': 10, 'colsample_bytree': 0.7762471807599582, 'subsample': 0.7167545683373836, 'eta': 0.07549059814774908}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:33,750]\u001b[0m Trial 79 finished with value: 0.11314771727105759 and parameters: {'min_child_weight': 157, 'alpha': 1.6080476205942777e-06, 'max_depth': 9, 'colsample_bytree': 0.9235769517151257, 'subsample': 0.6715537917418835, 'eta': 0.05542989487820416}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:39,532]\u001b[0m Trial 80 finished with value: 0.11387291428192073 and parameters: {'min_child_weight': 65, 'alpha': 5.9340470509201485e-06, 'max_depth': 8, 'colsample_bytree': 0.9893485346680899, 'subsample': 0.6353960331331736, 'eta': 0.10713420338173213}. Best is trial 54 with value: 0.10629498010912801.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:45,714]\u001b[0m Trial 81 finished with value: 0.10314301931006332 and parameters: {'min_child_weight': 20, 'alpha': 0.000482573369062248, 'max_depth': 8, 'colsample_bytree': 0.9942528957377031, 'subsample': 0.7486478686740458, 'eta': 0.08912579168750279}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:51,986]\u001b[0m Trial 82 finished with value: 0.10706818209164053 and parameters: {'min_child_weight': 54, 'alpha': 0.0010612899597393166, 'max_depth': 8, 'colsample_bytree': 0.962261949616073, 'subsample': 0.6980793941255582, 'eta': 0.0825092726140393}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:55:58,087]\u001b[0m Trial 83 finished with value: 0.10839868568679997 and parameters: {'min_child_weight': 51, 'alpha': 0.0007865142110033437, 'max_depth': 8, 'colsample_bytree': 0.9588648470399843, 'subsample': 0.7023292967297131, 'eta': 0.08498972426604104}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:04,245]\u001b[0m Trial 84 finished with value: 0.10629387465905638 and parameters: {'min_child_weight': 52, 'alpha': 0.005980029913518092, 'max_depth': 8, 'colsample_bytree': 0.9791927533611571, 'subsample': 0.7025553209664203, 'eta': 0.1001526662369457}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:10,209]\u001b[0m Trial 85 finished with value: 0.1102222951733761 and parameters: {'min_child_weight': 50, 'alpha': 0.0052561261862477185, 'max_depth': 8, 'colsample_bytree': 0.9616906779038246, 'subsample': 0.6573919145968237, 'eta': 0.08547248914954346}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:16,655]\u001b[0m Trial 86 finished with value: 0.11257561022439184 and parameters: {'min_child_weight': 33, 'alpha': 0.00554605392864492, 'max_depth': 8, 'colsample_bytree': 0.9810201619737207, 'subsample': 0.8368377800146034, 'eta': 0.09934027815981618}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:22,642]\u001b[0m Trial 87 finished with value: 0.1052697160125011 and parameters: {'min_child_weight': 45, 'alpha': 0.0009735262632607881, 'max_depth': 8, 'colsample_bytree': 0.9993704423122391, 'subsample': 0.7486266586885594, 'eta': 0.11064705492814955}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:28,606]\u001b[0m Trial 88 finished with value: 0.11264276465016836 and parameters: {'min_child_weight': 73, 'alpha': 0.009170931903787753, 'max_depth': 8, 'colsample_bytree': 0.9980976721370423, 'subsample': 0.74628188174098, 'eta': 0.11308478983986137}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:34,386]\u001b[0m Trial 89 finished with value: 0.11263524395048465 and parameters: {'min_child_weight': 39, 'alpha': 0.0011483019510284962, 'max_depth': 7, 'colsample_bytree': 0.9778140885897169, 'subsample': 0.780844472600016, 'eta': 0.18547492174544764}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:40,060]\u001b[0m Trial 90 finished with value: 0.11173252270594324 and parameters: {'min_child_weight': 130, 'alpha': 0.0017206547765439794, 'max_depth': 8, 'colsample_bytree': 0.9860705998429192, 'subsample': 0.7180623576414631, 'eta': 0.10380553624881271}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:46,370]\u001b[0m Trial 91 finished with value: 0.10789068174622012 and parameters: {'min_child_weight': 51, 'alpha': 0.000526346799285351, 'max_depth': 8, 'colsample_bytree': 0.9551747454258144, 'subsample': 0.6984842915348917, 'eta': 0.09270923500198736}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:52,462]\u001b[0m Trial 92 finished with value: 0.10711727963541155 and parameters: {'min_child_weight': 47, 'alpha': 0.0004900681760609371, 'max_depth': 8, 'colsample_bytree': 0.9406756904750233, 'subsample': 0.7668666150316543, 'eta': 0.09491995111943768}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:56:58,207]\u001b[0m Trial 93 finished with value: 0.11603648604080684 and parameters: {'min_child_weight': 30, 'alpha': 0.054798955151784026, 'max_depth': 8, 'colsample_bytree': 0.9464647650291709, 'subsample': 0.6777028222113536, 'eta': 0.1156162413571108}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:57:03,893]\u001b[0m Trial 94 finished with value: 0.10917322145387788 and parameters: {'min_child_weight': 50, 'alpha': 0.0005168438564828197, 'max_depth': 8, 'colsample_bytree': 0.9674340385914472, 'subsample': 0.7963184681005697, 'eta': 0.09691543748376075}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:57:09,816]\u001b[0m Trial 95 finished with value: 0.1105650589628897 and parameters: {'min_child_weight': 30, 'alpha': 0.0025988660427713243, 'max_depth': 8, 'colsample_bytree': 0.9404960987984715, 'subsample': 0.7618758128576553, 'eta': 0.07215344497360783}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:57:15,465]\u001b[0m Trial 96 finished with value: 0.11059358118294252 and parameters: {'min_child_weight': 66, 'alpha': 0.0004700259983570644, 'max_depth': 7, 'colsample_bytree': 0.997785531171125, 'subsample': 0.732083821986281, 'eta': 0.15406714713109027}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "\u001b[32m[I 2022-04-20 01:57:21,870]\u001b[0m Trial 97 finished with value: 0.11350048905956353 and parameters: {'min_child_weight': 45, 'alpha': 0.0009370681558424233, 'max_depth': 8, 'colsample_bytree': 0.9797670058653873, 'subsample': 0.6934754609238836, 'eta': 0.12894274976691963}. Best is trial 81 with value: 0.10314301931006332.\u001b[0m\n",
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    }
   ],
   "source": [
    "results, study = search_boosting(mypipe, features_train, features_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1229.970786942123,\n",
       " {'min_child_weight': 21,\n",
       "  'alpha': 4.942615434527387e-06,\n",
       "  'max_depth': 13,\n",
       "  'colsample_bytree': 0.9188509285361383,\n",
       "  'subsample': 0.8928671489358959,\n",
       "  'eta': 0.16142476850744988})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "study.best_trial.value, study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fpala\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    }
   ],
   "source": [
    "params =  {\n",
    "    'min_child_weight': 21,\n",
    "    'alpha': 4.942615434527387e-06,\n",
    "    'max_depth': 13,\n",
    "    'colsample_bytree': 0.9188509285361383,\n",
    "    'subsample': 0.8928671489358959,\n",
    "    'eta': 0.16142476850744988\n",
    "  }\n",
    "pipe = Pipeline(steps=[\n",
    "            ('scaler', PowerTransformer()),\n",
    "            ('xgb', XGBRegressor(**params)),\n",
    "])\n",
    "pipe.fit(features_train,target_train)\n",
    "preds_test = pipe.predict(features_test)\n",
    "preds_train = pipe.predict(features_train)\n",
    "mse_test = mean_squared_error(y_true=target_test, y_pred=preds_test)\n",
    "mse_train = mean_squared_error(y_true=target_train, y_pred=preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 14.273936728016926\n",
      "mse test:  1229.970786942123\n",
      "rmse test:  35.07093935072346\n"
     ]
    }
   ],
   "source": [
    "print('mse train:', mse_train)\n",
    "print('mse test: ', mse_test)\n",
    "print('rmse test: ', np.sqrt(mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = pd.read_csv('../data/processed/future.csv')\n",
    "future_features = future_data.drop(columns=['symbol', 'calendarYear', 'fillingDate', 'target', 'targetYoY'])\n",
    "future_target = future_data.targetYoY\n",
    "preds = pipe.predict(future_features)\n",
    "mse = mean_squared_error(y_true=future_target, y_pred=preds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1af52d91e9a17fda2c5a861893e28b9d34197fc091af0d7ff917999a49fdd885"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
